{
	"video_001": {
		"section": "Week 1 - Introduction to Computer Vision",
		"subsection": "Video: Introduction to Computer Vision",
		"unit": "Introduction to Computer Vision",
		"video_sources": [
			"https://edx-video.net/3b084935-e2f1-4c3c-aac3-00d3b7d60db0.m3u8",
			"https://edx-video.net/3b084935-e2f1-4c3c-aac3-00d3b7d60db0-mp4_720p.mp4"
		],
		"video_duration": 249,
		"speech_period": [
			6.17, 3.265, 1.965, 1.56, 1.755, 1.755, 1.5, 2.35, 1.97, 2.355, 1.665,
			3.36, 1.89, 3.6, 2.73, 1.95, 2.82, 3.15, 1.53, 3.45, 2.45, 2.415, 2.405,
			3.04, 1.755, 1.755, 2.385, 1.425, 1.83, 3.255, 2.355, 2.805, 2.445, 2.465,
			1.585, 3.03, 1.83, 2.325, 3.21, 3.715, 2.174, 2.331, 2.22, 1.275, 1.8,
			1.62, 2.22, 2.73, 2.85, 2.94, 1.89, 2.165, 2.125, 1.86, 2.1, 3.15, 2.055,
			2.355, 2.76, 2.43, 1.08, 2.175, 1.635, 1.365, 1.155, 2.61, 2.85, 2.79,
			3.75, 2.43, 2.82, 1.95, 2.51, 2.8, 2.67, 2.1, 1.71, 2.37, 1.74, 2.775,
			2.655, 2.81, 2.77, 3.495, 2.415, 2.01, 3.11, 1.81, 3.24, 2.79, 2.865,
			3.15, 1.305, 3.09, 1.845, 2.055, 2.94, 1.98, 2.625, 2.715, 3.82, 3.9
		],
		"transcript_en": [
			"(Music)",
			"Welcome to introduction to Computer Vision.",
			"In this course, you will get introduced to",
			"the field of computer vision,",
			"and we'll learn about its applications.",
			"Further, you will also learn about",
			"the specialized methods and",
			"techniques that make up the field of computer vision,",
			"and we'll learn to implement these techniques",
			"using Python. So let's get started.",
			"After completing this course,",
			"you will: understand what is computer vision,",
			"be able to apply and use",
			"computer vision algorithms with Python and OpenCV,",
			"know how to create your own custom classifiers",
			"that can be applied to business problems,",
			"build your own web app to classify images.",
			"What you will not learn, how computer vision works,",
			"how neural networks or",
			"deep learning work, math and statistics.",
			"To start, let's begin by understanding by",
			"what exactly is computer vision?",
			"What do you see in the following image?",
			"We can clearly see that this is a picture of a giraffe.",
			"As humans, when we see an image,",
			"we can instantaneously recognize",
			"the contents of the image and interpret it.",
			"Computers on the other hand,",
			"aren't capable of doing so.",
			"As you may have guessed, computer vision is about this.",
			"It is providing computers the ability to",
			"see and understand images.",
			"Now let's understand why computer vision",
			"is creating big waves in the industry.",
			"At a high level, the reason",
			"why computer vision or any technology is adopted,",
			"is because of some improvement.",
			"It could be that something slower becomes",
			"significantly faster, expensive becomes cheaper,",
			"manual becomes automated, difficult becomes easy,",
			"inconvenient becomes convenient,",
			"unscalable becomes scalable.",
			"One really groundbreaking change",
			"as we've seen in the news,",
			"is self-driving cars which",
			"frees up time for passengers through",
			"automation and can potentially",
			"save more lives than having human drivers.",
			"But in the course, we'd like to focus on smaller,",
			"more narrow solutions that use computer vision.",
			"Computer vision has applications in",
			"all industries and sectors.",
			"We have seen that computer vision has made",
			"an enormous impact especially",
			"in industries like automotive,",
			"manufacturing, human resources, insurance,",
			"healthcare, and many more.",
			"In fact, there isn't an industry where",
			"computer vision does not have an application.",
			"Now, let's dive right into how",
			"computer vision has been",
			"changing and disrupting industries.",
			"If you don't know much about",
			"the oil and gas industry",
			"and you're not familiar with what's",
			"happening in the United Arab Emirates then you",
			"probably won't know of this company called ADNOC,",
			"that produces about 3 million barrels of oil and",
			"10.5 billion cubic feet of raw gas each day.",
			"Founded in 1971,",
			"the Abu Dhabi National Oil Company or",
			"ADNOC is a diversified group",
			"of energy and petrochemical companies.",
			"This Emirati firm has previously",
			"used a labor intensive process for classifying",
			"the characteristics of rock samples",
			"requiring precious time and",
			"energy from expert geologists.",
			"After geologists fed",
			"high resolution rock images into a database,",
			"they used IBM Watson to analyze and",
			"properly identify classes of carbonate rock,",
			"giving ADNOC the ability to classify up to",
			"25,000 thin section rock images per day.",
			"With Watson, they can run a set of images",
			"for a whole reservoir in minutes,",
			"saving their expert geologists precious time.",
			"Let's look at another case of how",
			"computer vision is revolutionizing the hiring process.",
			"In the HR world, computer vision is changing",
			"how candidates get hired in the interview process.",
			"Knockri, a Canadian based startup,",
			"is making waves with",
			"their AI video soft skill assessment tool.",
			"By using computer vision,",
			"machine learning and data science,",
			"they're able to quantify soft skills and conduct",
			"early candidate assessments to",
			"help large companies shortlist the candidates.",
			"In this video, we saw how applications of",
			"computer vision are impacting our everyday lives.",
			"(Music)"
		],
		"is_youtube": false
	},
	"video_002": {
		"section": "Week 1 - Introduction to Computer Vision",
		"subsection": "Video: Applications of Computer Vision",
		"unit": "Applications of Computer Vision",
		"video_sources": [
			"https://edx-video.net/786b3b08-ba15-4940-bdca-b436487b2340.m3u8",
			"https://edx-video.net/786b3b08-ba15-4940-bdca-b436487b2340-mp4_720p.mp4"
		],
		"video_duration": 203,
		"speech_period": [
			7.063, 3.604, 3.003, 1.86, 2.746, 3.554, 1.95, 1.7, 3.04, 3.96, 3.14,
			4.31, 1.05, 5.05, 3.99, 2.69, 4.1, 3.069, 3.642, 3.109, 4.28, 2.97, 2.67,
			2.85, 4.37, 0.92, 1.77, 4.48, 5.04, 2.19, 3.42, 3.1, 4.62, 3.53, 3.46,
			5.36, 4.55, 2.36, 2.62, 4.28, 7.23, 3.01, 5.24, 2.83, 4.61, 3.83, 4.95,
			4.56, 2.53, 4.51, 2.39, 3.7, 4.11, 3.602, 1.139, 5.159
		],
		"transcript_en": [
			"[MUSIC]",
			"In this video, we'll talk about some of the applications of computer vision and",
			"how you can come up with some applications yourself.",
			"What about sifting through videos?",
			"Have you ever tried looking for particular scene of a movie and",
			"had to to fast forward to skip around until you could find it?",
			"What about your own videos?",
			"What if they could be searchable?",
			"IBM created an example of how you could do this",
			"by tagging videos with keywords based on the objects that appear in each scene.",
			"Now imagine being that security company who's asking to look for",
			"a suspect in a blue van amongst hours and hours of footage.",
			"With computer vision and",
			"object recognition, searching through videos has become a thing of the past.",
			"Now, other companies like insurance and civil engineering companies",
			"may have a completely set of different problems they want to tackle.",
			"Take for example this electric towers which require some degree of maintenance",
			"to check for degrees of rust and other structural defects.",
			"Certainly manually climbing up the tower to scour every inch and",
			"corner would be extremely time consuming and costly.",
			"Flying a drone with wires around doesn't sound particularly safe either.",
			"So how could you apply computer vision here?",
			"Well imagine that if you a person on the ground",
			"took high resolution images from different angles.",
			"Then, a computer vision specialist could cut up the images into a grid of",
			"smaller images.",
			"With each of the smaller images,",
			"you could then develop a custom image classifier that detects the presence of",
			"metal structure versus other structure versus non metal objects.",
			"With this custom metal classifier,",
			"you could then determine which areas of the image contain metal.",
			"But that wouldn't be enough to just detect metal.",
			"You could then create another custom classifier to determine the level of rust",
			"based on certain structural guidelines or criteria.",
			"So for example, you could have grade 1 rust,",
			"which cold be very minimal rust to grade 6 rust, which would be more severe.",
			"And finally, by sending the image through the second classifier to detect rust,",
			"it could then determine the level of rust for",
			"certain areas of the image that contain metal.",
			"Using metal classifiers and rust classifiers when scaled out across",
			"thousands or millions of images could save insurance companies millions of dollars.",
			"Another way to use such classifiers would be to directly map",
			"the areas of concern onto the image like this picture of a rusted bridge.",
			"You could create a grid like heat map",
			"to establish which areas are classified as rust on the image.",
			"Furthermore, insurance companies often have to worry about grading the severity",
			"of certain claims, which could be a little difficult for people to classify.",
			"For example, the image of the left contains small areas of damage from hail",
			"and ice pellets during a storm.",
			"And with computer vision, damage could much more easily classified.",
			"This could potentially, save a lot of time and",
			"money spent in processing each insurance claim.",
			"In this video, we saw some of the applications of computer vision in action",
			"and how computer vision techniques are used across industries.",
			"Thank you for watching.",
			"[MUSIC]"
		],
		"is_youtube": false
	},
	"video_003": {
		"section": "Week 1 - Introduction to Computer Vision",
		"subsection": "Video: Recent Research in Computer Vision",
		"unit": "Recent Research in Computer Vision",
		"video_sources": [
			"https://edx-video.net/c34d4301-9620-421a-94f0-5f38d6db885b.m3u8",
			"https://edx-video.net/c34d4301-9620-421a-94f0-5f38d6db885b-mp4_720p.mp4"
		],
		"video_duration": 278,
		"speech_period": [
			6.05, 3.49, 1.785, 1.53, 2.115, 2.535, 2.865, 1.29, 2.1, 1.32, 3.33, 1.92,
			2.13, 3.51, 3.18, 3.59, 1.5, 2.295, 2.775, 3.3, 4.2, 3.36, 3.3, 3.195,
			3.075, 1.98, 3.66, 3.52, 1.61, 2.83, 5.0
		],
		"transcript_en": [
			"(Music)",
			"Now, let's talk about some active research that has been",
			"happening in the field of computer vision",
			"over the past decade.",
			"Researchers at Facebook are",
			"working on detecting objects in images.",
			"Detecting objects correctly and efficiently in",
			"images is an extremely",
			"important field in computer vision.",
			"This is because to make",
			"meaningful inferences in images and video streams,",
			"computers need to correctly detect",
			"objects in them as a first step.",
			"With the recent rise in popularity of self-driving cars,",
			"detecting objects in video streams has become important.",
			"Since videos from camera feeds of those cars need to be",
			"analyzed for the car to avoid",
			"obstacles and prevent collisions.",
			"How cool is it to convert an image of",
			"a horse to a zebra and vice versa,",
			"or to change the scene of an image from summer to winter?",
			"Image to image translation is about this.",
			"The UC Berkeley Research Team is working on converting",
			"an image from one representation of a scene to another.",
			"Everybody Dance Now is a project that uses",
			"computer vision techniques for",
			"performing motion transfer from one subject to another.",
			"Given we have a video of a person performing dance moves,",
			"the person's dance moves can",
			"then be transferred to an amateur target.",
			"(Music)"
		],
		"is_youtube": false
	},
	"video_004": {
		"section": "Week 1 - Introduction to Computer Vision",
		"subsection": "Video: Brainstorming Your Own Applications",
		"unit": "Brainstorming Your Own Applications",
		"video_sources": [
			"https://edx-video.net/1121ef6e-d16f-4f64-8847-0a1ef652192f.m3u8",
			"https://edx-video.net/1121ef6e-d16f-4f64-8847-0a1ef652192f-mp4_720p.mp4"
		],
		"video_duration": 206,
		"speech_period": [
			2.29, 1.72, 2.21, 2.52, 2.295, 1.62, 1.575, 1.53, 2.04, 1.44, 2.265,
			2.035, 1.45, 2.76, 3.255, 1.995, 2.265, 3.15, 3.045, 2.13, 3.0, 3.48,
			1.68, 3.11, 2.5, 2.67, 3.72, 1.56, 2.04, 2.25, 1.875, 2.97, 1.5, 1.68,
			4.05, 2.685, 2.49, 1.53, 3.66, 2.045, 1.915, 1.575, 2.16, 2.744, 3.046,
			2.01, 2.235, 1.859, 2.041, 2.2, 3.02, 3.155, 2.36, 2.59, 2.335, 2.67,
			2.37, 1.44, 3.22, 2.7, 1.49, 1.53, 1.47, 1.955, 2.265, 1.29, 1.815, 1.655,
			2.575, 2.04, 3.15, 2.475, 2.34, 1.62, 1.8, 1.56, 2.25, 1.885, 2.46, 3.695,
			2.88, 3.585, 2.47, 3.805
		],
		"transcript_en": [
			"So what are some ideas that",
			"you might be able to come up with?",
			"In this course, you'll have opportunities to",
			"create your own computer vision applications.",
			"So it's good to start thinking about it now.",
			"I find it most helpful If you",
			"start not from the solution,",
			"but with an existing problem.",
			"So instead of saying something like,",
			"I want to create, use",
			"computer vision to recognize different food.",
			"You should think, home,",
			"visually impaired people have",
			"difficulty identifying food in a grocery store.",
			"How could I use computer vision to tackle that problem?",
			"So let's think about somehow",
			"to identify existing problems.",
			"It often helps if we narrow it down by industry.",
			"Let's start with the easier ones we can relate to.",
			"In the field of medicine, what is one of",
			"the most difficult and time-consuming tasks?",
			"Training doctors to accurately detect cancer,",
			"training and developing",
			"that expertise. What about driving?",
			"Well, certainly driving is an easy one because",
			"we can't look at everything at the same time.",
			"Driving requires constant focused visual attention,",
			"and the consequences are pretty",
			"severe if mistakes are made.",
			"Furthermore, long-distance drivers are",
			"prone to falling asleep at the wheel.",
			"What about the security and surveillance industry?",
			"We talked about this earlier,",
			"but looking for a suspect across",
			"hours and hours of footage consumes a lot of time.",
			"On a different note, detecting dangerous goods",
			"in X-rays at the airport is difficult,",
			"and the consequences of making",
			"a mistake could pose significant security risks.",
			"In the manufacturing industry,",
			"thousands or tens of thousands of",
			"products could be built",
			"or assembled in some supply chains.",
			"Trying to make sure each item is of high-quality",
			"could be very difficult and labor-intensive.",
			"How might computer vision help?",
			"Furthermore, from an insurance",
			"and work safety perspective,",
			"often construction employees are",
			"found not wearing appropriate safety helmets,",
			"equipment, or vests when working with heavy machinery.",
			"How might computer vision help ensure compliance?",
			"In addition for insurance,",
			"car accidents happen all the time,",
			"but evaluating the damage for each car in",
			"the accident takes time and expertise.",
			"It also depends on the make, model,",
			"and year of the car among",
			"other things to establish cost of replacement.",
			"How might computer vision help there?",
			"So hopefully that has given you",
			"some ideas to start thinking about",
			"the ways computer vision could",
			"be used in various industries,",
			"and how you might be able to start thinking of",
			"your own problems that could",
			"be solved with computer vision.",
			"To make it easier for you,",
			"think about the problems you face in your job,",
			"at home, or with friends.",
			"What problems could computer vision solve?",
			"Do you ever have problems finding receipts for",
			"items you want to return or exchange?",
			"Do you ever wonder what kind of",
			"plant you have in your backyard,",
			"or how you might be able to keep",
			"track of the growth of your plants at home?",
			"Do you have rodent problems?",
			"Or perhaps like we do in Toronto, Canada,",
			"problems with raccoons diving into our garbage bins?",
			"There are endless possibilities with computer vision.",
			"So start asking around to see if you can make your life,",
			"your friends lives, or your work",
			"better or more convenient with computer vision."
		],
		"is_youtube": false
	},
	"video_005": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: What Is A Digital Image",
		"unit": "What Is A Digital Image",
		"video_sources": [
			"https://edx-video.net/924d7785-e4fa-42c3-a497-635b0ae7fcee.m3u8",
			"https://edx-video.net/924d7785-e4fa-42c3-a497-635b0ae7fcee-mp4_720p.mp4"
		],
		"video_duration": 472,
		"speech_period": [
			3.175, 3.675, 2.975, 2.485, 2.445, 1.695, 1.59, 3.215, 2.24, 1.665, 2.88,
			2.01, 2.16, 3.175, 2.285, 2.25, 1.41, 2.975, 2.275, 3.915, 2.925, 3.045,
			2.925, 2.37, 3.045, 3.59, 3.24, 2.105, 3.485, 1.83, 1.84, 1.535, 2.37,
			3.0, 1.62, 2.225, 2.735, 1.86, 2.485, 2.25, 2.84, 2.3, 2.745, 2.175,
			1.805, 3.845, 4.185, 3.525, 1.395, 2.13, 3.27, 1.935, 1.515, 2.38, 2.495,
			1.775, 2.545, 3.44, 3.535, 3.805, 1.23, 1.665, 2.535, 2.205, 1.6, 2.245,
			2.13, 3.475, 1.56, 3.015, 3.165, 3.825, 2.625, 3.375, 2.685, 3.175, 2.035,
			2.575, 2.845, 4.04, 2.715, 3.675, 3.03, 1.935, 3.375, 2.624, 3.886, 2.725,
			2.09, 2.34, 2.36, 2.515, 2.46, 2.93, 2.9, 2.505, 2.82, 1.965, 2.415,
			2.865, 3.1, 1.78, 2.37, 1.92, 2.415, 2.4, 1.945, 2.35, 2.7, 3.3, 3.575,
			2.435, 2.475, 2.055, 2.4, 2.715, 2.985, 2.675, 1.83, 2.725, 3.24, 3.21,
			3.795, 1.875, 3.525, 2.415, 2.62, 2.42, 2.42, 3.26, 1.65, 1.53, 1.38,
			1.81, 2.32, 3.77, 2.895, 1.905, 2.565, 3.36, 4.145, 2.485, 2.665, 3.51,
			2.585, 2.515, 2.51, 3.8, 4.01, 2.67, 2.17, 2.915, 3.615, 2.42, 2.91, 2.58,
			3.69, 2.13, 2.17, 1.58, 2.385, 2.385, 2.685, 1.8, 2.34, 3.18, 3.81, 3.89,
			2.085, 2.565, 2.655, 1.845, 2.955, 2.775, 4.255
		],
		"transcript_en": [
			"What is a digital image?",
			"In this video, we will discuss what is a Digital Image.",
			"Processing digital images in Python.",
			"A digital image can be interpreted",
			"as a rectangular array of numbers.",
			"In many cases it's easier",
			"to understand a gray-scale image,",
			"an image that is made up of different shades of gray.",
			"If we zoom into the region,",
			"we see the image is comprised of",
			"a rectangular grid of blocks called pixels.",
			"We can represent these pixels with",
			"numbers called intensity values.",
			"We overlay the intensity values over the pixels.",
			"In the real world, an image can take on",
			"an almost unlimited number of values,",
			"but digital images have",
			"intensity values between zero and 255.",
			"It turns out that's all we need,",
			"256 different intensity values to represent an image.",
			"The following bar demonstrates the relationship between",
			"the different shades of gray and the numerical values.",
			"Darker shades of gray have lower values with",
			"zero as black and lighter shades",
			"have higher values with 255 as white.",
			"The contrast is the difference between these values.",
			"Let's see what happened if we use less values.",
			"Consider the following image,",
			"on the left, we have 256 intensity values.",
			"On the right, we have 32.",
			"The images look similar.",
			"Let's reduce the number of",
			"intensity values on the right image.",
			"Here we have 16 intensity values.",
			"You can start seeing a difference",
			"in regions of low contrast.",
			"Here we have eight intensity values",
			"and the image looks off.",
			"Here we have two intensity values.",
			"The image looks like a cartoon.",
			"At the end we have an array of numbers,",
			"the height is the number of rows,",
			"in this case 500 as shown in the plot,",
			"and width is the number of columns,",
			"in this case 500.",
			"It's like a rectangle or in this case, a square.",
			"Each pixel or intensity value has its own index.",
			"For rows, we start at the top of the image and move down.",
			"For columns, we start at",
			"the left of the image and move right.",
			"Each pixel value comes from a grid of sensors.",
			"We have the original object.",
			"The image is the quantized",
			"samples obtained from the grid.",
			"Consider the following image,",
			"it's a combination of red,",
			"blue and green images,",
			"it's sometimes referred to as an RGB image.",
			"This is just one of many color representations.",
			"These color values are represented as different channels.",
			"Like the gray-scale image,",
			"each channel is an image.",
			"We have the red channels intensity values,",
			"the blue channels intensity values",
			"and the green channels.",
			"If a gray-scale image is a square,",
			"a color image is like a cube.",
			"Each one of these channels has its own intensity values.",
			"In addition to accessing",
			"each image intensity with a row and column index,",
			"we also have an index for each channel, in this case,",
			"zero for red, one for blue and two for green.",
			"We can have black and white images.",
			"Here we have an image mask used to identify objects.",
			"The intensities corresponding to the person are",
			"represented with one and the rest are zeros.",
			"The display adjusts contrast,",
			"so zero is black and one is white.",
			"The video sequence is a sequence of images.",
			"Here we have five images representing five video frames.",
			"For the first frame we have,",
			"for the second frame we have, and so on.",
			"An image is a file on your computer.",
			"Two popular image formats,",
			"Joint Photographic Expert Group image or JPEG,",
			"and Portable Network Graphics or PNG,",
			"these formats reduce file size and have other features.",
			"No matter what Python library you use,",
			"you're going to have to load the image.",
			"If your code is in the same directory,",
			"all you need is the name of the file.",
			"If your code is in a different directory,",
			"you will need the path of the file.",
			"The pillow, or PIL library is",
			"a popular library for working with images in Python.",
			"We will use it for some tasks.",
			"We import the image module from PIL,",
			"you can load the image as follows",
			"and create a PIL image object.",
			"You can plot the image using show but we",
			"will use matplotlibs imshow.",
			"The attribute format is",
			"the extension or format of the image.",
			"The attribute size is the number of",
			"pixels that make up the width and height.",
			"The attribute mode is the color space,",
			"in this case, RGB.",
			"ImageOps module contains several",
			"ready-made image processing operations.",
			"We can convert the image to gray-scale as follows,",
			"the image mode is L. This means luminance.",
			"We can save the gray-scale image and we",
			"can convert it from PNG to JPEG.",
			"We can load gray-scale images.",
			"The image mode is L. We can",
			"use the quantize method to quantize the image.",
			"The input is the number of quantization levels.",
			"You can also work with the different color channels.",
			"Consider the following image,",
			"we can obtain the different RGB channels as",
			"gray-scale images and assign them to the variables red,",
			"green and blue using the method split.",
			"We can plot the Red Channel as a gray-scale image.",
			"We can see that the red pixels and",
			"the gray-scale plot of the red have a higher intensity.",
			"We can do the same for the blue channel.",
			"The relationship is the same.",
			"There are several ways to convert",
			"a PIL image to a numpy array.",
			"For example, we can use the array constructor,",
			"we can print the array.",
			"These arrays are similar to",
			"the OpenCV arrays that",
			"we will review in the next section.",
			"Check out the lab for more.",
			"OpenCV is a library used for computer vision.",
			"It has more functionality than the PIL library,",
			"but is more difficult to use.",
			"This will be our main focus.",
			"We can import OpenCV as follows,",
			"the imread method loads an image from the specified file.",
			"The input is the path of the image.",
			"The result is a numpy array with",
			"intensity values as 8-bit, unassigned data types.",
			"We can obtain the shape of the array.",
			"We can plot the image using Imshow,",
			"but the colors appear off.",
			"This is because the order of each channel is different in",
			"OpenCV unlike PIL that is RGB.",
			"OpenCV is BGR.",
			"This is the main difference between the arrays",
			"and PIL versus OpenCV.",
			"We can change the color space with conversion code,",
			"this changes the color space.",
			"We use the function cvtColor,",
			"the input is the color image and",
			"the color code BGR to RGB or blue,",
			"green, red to red, green, blue.",
			"We can now plot the image.",
			"You can also convert the image",
			"to gray-scale using cvtColor.",
			"The input is the original image",
			"and the BGR to gray color code.",
			"We can plot the image.",
			"We can save the image using imright,",
			"the input is the path and the image array.",
			"We can also load gray-scale images in OpenCV.",
			"We must specify the color code for imread color scale.",
			"We can load a color image and use",
			"slices to obtain the different color channels.",
			"We assign each channel to the corresponding array,",
			"blue, green and red.",
			"Slicing selects each row and column.",
			"The final index selects the color channel.",
			"We can view the color channels blue, green and red."
		],
		"is_youtube": false
	},
	"video_006": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: Manipulating Images",
		"unit": "Manipulating Images",
		"video_sources": [
			"https://edx-video.net/667c4699-80ed-4041-a04c-cc636777db5e-mp4_720p.mp4",
			"https://edx-video.net/667c4699-80ed-4041-a04c-cc636777db5e.m3u8"
		],
		"video_duration": 174,
		"speech_period": [
			4.0, 2.633, 4.0, 4.0, 5.667, 6.333, 5.733, 6.267, 6.467, 5.533, 5.967,
			8.133, 5.9, 4.0, 6.3, 6.833, 8.567, 8.267, 5.466, 4.567, 4.0, 7.267,
			4.733, 5.833, 6.967, 7.2, 8.0, 8.867, 2.433, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will discuss Manipulating Images",
			"Copying allows you to create a new image independent of the original,",
			"Consider the following image array using PIL or Open CV",
			"we can use the id function to find the object's memory address. If we assign the baboon array to \u201cA\u201d",
			"Using the id function we see the memory address is the same as the original array.",
			"If we apply the method copy(),  and assign it to B we see the the memory address is different",
			"Each column in the following table displays the image arrays and its corresponding  memory address",
			"If we set all the elements to the baboon array to zero using the following lines of code",
			"the array \"A\" will change as it points to the same locations in memory as baboon",
			"The result is both the baboon array and array \u201cA\u201d will be zero, but because we used the copy method array \u201cB\u201d will be unaffected",
			"You don\u2019t have to copy images all the the time but if you see this behavior in your code, it is a common mistake",
			"Flipping images changes the images orientation",
			"We can flip an image by changing the index value of a pixel or intensity, consider the following array",
			"If we convert the column indexes to row index\u2019s the image will have a different orientation or will become  flipped",
			"for example, if we overlay the following image where the second and fifth columns are comprised of several white lines after we convert the indexes",
			"the vertical lines will appear horizontal. For color images we can convert  all the color channels at the same time",
			"Let's try PIL. PIL has several ways to flip an image,",
			"we can use the\u00a0ImageOps\u00a0module: we can flip the image using the flip function",
			"we can also mirror the image using the mirror function",
			"we can use the\u00a0transpose()\u00a0method; The Image module has built-in attributes to describe the type of flip,",
			"in the case of the FLIP_TOP_BOTTOM function, we flip the image upside down",
			"Let us look at how to flip images using open CV",
			"OpenCV has several ways to flip an image, we can use the\u00a0\u201dflip\u201d\u00a0function; we have the input image array.",
			"The parameter is the\u00a0flipCode. If the parameter flipcode \u00a0= 0: it will  flip vertically around the x-axis",
			"We can also use the\u00a0rotate()\u00a0function ; OpenCV module has built-in attributes to describe the type of flip,",
			"the values are represented by integers. In the case of the Rotate 90 Clockwise, it ROTATEs the input image, 90 degrees CLOCKWISE",
			"Check out the labs for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_007": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: Manipulating Images One Pixel At a Time",
		"unit": "Manipulating Images One Pixel At a Time",
		"video_sources": [
			"https://edx-video.net/8ac268db-dd1a-49dc-b7cc-7c1b3d4efc79-mp4_720p.mp4",
			"https://edx-video.net/8ac268db-dd1a-49dc-b7cc-7c1b3d4efc79.m3u8"
		],
		"video_duration": 298,
		"speech_period": [
			6.633, 4.967, 4.233, 6.433, 7.067, 6.567, 2.733, 4.0, 4.0, 4.0, 4.0, 4.6,
			3.4, 6.9, 6.5, 5.1, 5.5, 4.0, 5.033, 7.534, 5.233, 3.1, 3.1, 6.567, 4.766,
			2.7, 6.734, 2.233, 6.2, 5.067, 5.733, 4.0, 3.3, 3.9, 3.067, 5.733, 7.567,
			9.933, 6.067, 7.133, 7.8, 5.5, 9.733, 6.267, 8.0, 5.067, 4.933, 4.233,
			4.634, 8.633, 8.5, 6.133, 8.334, 5.0
		],
		"transcript_en": [
			"",
			"In this video we will discuss how to Manipulate Images One Pixel At a Time",
			"We will discuss: Cropping, Changing Image Pixels",
			"Consider the following gray scale image each pixel or intensity is a value in an Array with its own index.",
			"For rows we start at the top of the image and move down. For columns we start At the left of the image and move right.",
			"Let\u2019s work on the following toy example. Instead of looking at the intensities lets look at the array index.",
			"We will call it array 1.",
			"Cropping is \"cutting out\" the part of the image and throwing out the rest;",
			"We can use slicing to select the rows 2 to 4",
			"This corresponds to the following elements In the image",
			"We can use slicing to select columns using 1 and 2",
			"The result is the following pixels, we can assign it to the variable A",
			"The new array has the following indexes",
			"We also perform slicing on multiple channels, the colon indicates to select each channel",
			"The result is the following rows and columns are selected, like before we can assign it to a new array.",
			"We can load PIL images and cv arrays",
			"We can a perform vertical crop; the variable\u00a0\u201cupper\u201d\u00a0is the first row that we would like to include in the image,",
			"the variable\u00a0\u201clower\u201d\u00a0is the last row we would like to include.",
			"We then use slicing to obtain the new image. We can also crop horizontally, .",
			"the variable right is the first column that we would like to include in the image, the variable left is the last column we would like to include in the image",
			"We can also apply crops to the PIL image objects",
			"We can also apply crops to the PIL image objects",
			"Consider the image intensities of all zeros,",
			"we can change the following elements to 255 by setting the array values in the array accordingly",
			"the results would be a rectangular box. We can draw simple shapes",
			"Consider the image intensities of all zeros",
			"we can set the following columns to 255  in row 4, we can also change the following elements",
			"the result looks like a face",
			"We can do the same thing for color image arrays, we specify the rows and columns we would like to change",
			"but we also specify the channel would like to change, in this case the red channel",
			"The following elements will change, the image will look like this",
			"We can also paste or super impose part of one image onto another",
			"we would like to add a blue box in image A to image I,",
			"we know the rows and columns the make up the blue box",
			"we can reassign those values to image I",
			"the result is the image values will be reassigned and the box will be on image I",
			"Let see some PIL. We can also use the PIL\u00a0\u201cImageDraw\u201d\u00a0 module to draw on PIL Image objects",
			"we will copy the image. The draw constructor creates the object  \u201cimage_fn\u201d this can be used to draw on the image \u201cimage_draw\u201d,",
			"We can draw a rectangle using the rectangle method, two important parameters include:\u00a0xy",
			"the coordinates bounding box and\u00a0fill\u00a0the Color of the rectangle. This will change the image object image_draw",
			"There are other shapes we can use for example, we can also overlay text on an image; we use the ImageFont module to obtain fonts",
			"We use the\u00a0text\u00a0method to place the text on the image. The parameters include\u00a0xy",
			"this is the top-left anchor coordinates of the text, the parameter text\u00a0the text to be drawn, and\u00a0fill\u00a0is the Color to use for the text.",
			"Consider the image \u201ccrop_image\u201d of a cat. We also have the \u201cimage_lenna \u201d,",
			"we can superimpose the image of the cat  over \u201cimage Lenna\u201d, all we need is the coordinates indicating where to paste the picture.",
			"We use the paste()\u00a0method, the input is the image we would like to super-impose and the box parameter specifies the top left corner of the image you would like to paste",
			"and the box parameter specifies the top left corner of the image you would like to paste",
			"We can also use open cv functions to perform pixel manipulations",
			"We can also create shapes and\u00a0openCV, we can use the method\u00a0 rectangle.",
			"The parameter\u00a0\u201dpt1\u201d \u00a0i.e left, top is the top-left coordinate of the rectangle, pause the video and take a look.",
			"\"pt2 ' is the bottom right coordinate right and lower point. Finally we have the color and thickness",
			"We can overlay text over an open CV array \u201cimage\u201d using the function\u00a0putText.",
			"The parameter text is the string you would like to overlay, org is the bottom-left corner of the text string. Check out the lab for more.",
			""
		],
		"is_youtube": false
	},
	"video_008": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: Pixel Transformations",
		"unit": "Pixel Transformations",
		"video_sources": [
			"https://edx-video.net/33d72fa6-469a-4ba4-aacb-ccfb79c29111-mp4_720p.mp4",
			"https://edx-video.net/33d72fa6-469a-4ba4-aacb-ccfb79c29111.m3u8"
		],
		"video_duration": 515,
		"speech_period": [
			6.633, 4.0, 4.7, 3.3, 6.167, 5.833, 4.0, 6.167, 5.833, 4.0, 4.0, 3.167,
			6.3, 3.933, 7.4, 6.567, 5.5, 4.6, 2.533, 6.5, 5.5, 3.267, 11.966, 6.567,
			6.2, 4.0, 7.467, 4.533, 4.0, 4.0, 4.0, 3.367, 5.066, 4.267, 8.4, 5.233,
			5.667, 4.0, 4.967, 4.066, 6.8, 7.234, 4.933, 4.0, 4.733, 5.2, 4.934,
			6.033, 7.1, 6.767, 6.7, 6.9, 5.9, 5.733, 8.0, 4.0, 8.0, 6.067, 3.366,
			4.534, 9.3, 5.533, 7.2, 4.0, 6.667, 5.333, 4.0, 5.333, 7.534, 7.633,
			6.967, 5.433, 3.1, 7.033, 4.967, 7.433, 5.434, 7.133, 6.967, 5.033, 7.833,
			4.167, 4.0, 4.0, 5.967, 4.9, 9.133, 8.0, 7.233, 2.334, 6.766, 5.0
		],
		"transcript_en": [
			"",
			"In this video we will discuss Pixel Transformations",
			"we will review: Histograms, Intensity Transformations",
			"Thresholding and Simple Segmentation",
			"We will use opencv as it has all the functions, the video will only focus on gray scale images",
			"Let\u2019s review histograms. A histogram counts the number of occurrences of a pixel,",
			"and it's a useful tool for understanding and manipulating images.",
			"Consider the following toy image with three intensity values, we can plot the occurrence of each pixel",
			" we have one black pixel, 5 gray pixels and two white pixels",
			"instead of counting the pixels, we can count the intensity values.",
			"Consider the following example as numbers ranging from 0 to 2,",
			"where 0 is black, 1 gray and 2 is white",
			"The histogram counts the pixel intensities. We represent these intensities as an array,",
			"the index of the array is the intensity level  r",
			"in most images we have 256 levels, representing the count of the different  intensity of gray levels",
			"consider the following image. We can calculate the histogram as follows. We have the image,",
			"the channel we would like to calculate, in this video we use the gray scale, the number of channels",
			"Finally we have the range of intensity values in yellow",
			"We can plot the histogram as a bar graph",
			"The darker portions represent the lower intensities, the brighter regions are mapped to the higher values",
			"An Intensity Transformation changes a image one pixel at a time",
			"Some image transformations depend on neighbouring pixels",
			"An Intensity Transformation T depends on only one single point\u00a0i,j, in the image array \u201cf\u201d, the image array \u201df\u201d is converted to array g",
			"The transform also operates on the intensity of gray levels \u201cr\u201d mapping them them to \u201cs\u201d",
			"This changes the histogram, lets apply a simple transform to a toy image",
			"We apply the following linear transform to the image array f,",
			"returning the array g, we apply the transform at the pixel located at the first row first column",
			"we get the following result at the same pixel location at array g",
			"We apply the transform at the pixel located at the second row first column",
			"We get the following result at the same index in array g",
			"We repeat the process for the entire array.",
			"Let's see how the transform changes the histogram",
			"We have the histogram each pixel intensity is a function of \u201cr\u201d or histogram r",
			"The transform maps each intensity value at r to s",
			" we have the new intensity values s and it\u2019s histogram s, let\u2019s see the relationship between histogram \u201cs\u201d and \u201cr\u201d",
			"the value of histogram r has one intensity value at zero(click 1)  the value of histogram r has one intensity  value at zero",
			"when applying the transform to r we see the intensity value of zero is mapped to 1",
			" the histogram s now has one intensity value at one",
			"we see the intensity value r =1 is mapped to  3",
			"the histogram r has a value of 5 at r=1",
			"In histogram s the value of 5 is mapped to s=3, and so on.",
			"If we plot the histogram after applying the transform, we see the histogram is shifted and scaled",
			"Image Negatives is where we reverse the intensity levels of an image",
			"Consider the following image, its difficult to make out details",
			"Applying the following transform will reverse the intensity levels of the image",
			"We can apply the transform as an array operation as follows",
			"The new image looks like this; we see the details are much more evident",
			"The following plot shows the histogram. We can overly the transform as a function of the intensity",
			"The function maps the lower intensity values of  0,1 and to 255, 254 and so on",
			"We can plot the value of the histogram on the horizontal axis, we see this flips the histogram",
			"A linear transform can be seen as applying Brightness and Contrast Adjustments",
			"We can use the following linear model where alpha is Simple contrast control and beta is Simple brightness control",
			"We can use the following values, just changing beta we can adjust the brightness",
			"Rather than using array operations we use the function   \u201cconvertScaleAbs\u201d after applying the transformation.",
			"The function scales, calculates absolute values, so the intensity values fall in the 0 to 255 value range",
			"We see the image is much brighter",
			"We see the histogram of the bright image has shifted to the right. A lot of  the intensity values have been mapped to the point value of 255",
			"We can adjust alpha too change the contrast. We see the contrast in the darker areas have improved",
			"but the lighter areas the image appears washed out",
			"Comparing the original image histogram to the histogram of the new image",
			"We see the lower values exhibit more spread, but many of the larger values have been mapped to 255 explaining why the image appeared washed out",
			"We can use nonlinear functions, but let\u2019s explore some algorithms that optimize contrast",
			"Histogram Equalization is an algorithm that uses the image's histogram to adjust contrast",
			"Consider, the following image with following histogram",
			"the function \u201cequalizeHist\u201d improves contrast, by using the histogram to determine a transform",
			"that flattens the histogram, the resulting image has improved contrast",
			"A threshold function applies a threshold to every pixel,",
			"it can be used in extracting objects from an image this is called segmentation",
			"The following function applies a threshold to the input array input image array imput_out and outputs the result",
			"the following portion of the code will cycle through each pixel\u00a0(\ud835\udc56,\ud835\udc57). If the pixel is greater then that threshold",
			"It will set a pixel in the array \u201cimage_out\u201d pixel to some value, usually 1 or 255,",
			"Otherwise it will set it to another value, usually zero. Let\u2019s do an example",
			"Consider the following array",
			"In this case the threshold value is 1 and the min value is 0 and the max value 255",
			"the first loop increments through the rows, for each column in \u201cinput_img\u201d",
			"we check  the condition if  the values greater than 1 we set the corresponding element  in \u201cimage_out\u201d to 255.",
			"Otherwise we set it to zeros. We repeat the process for the second row",
			"As none of the columns are less than one all the corresponding rows in \u201dImage_out\u201d is set to zero",
			"We repeat the process for the final row. The result is all the values in the new image array are white or black",
			"Consider the following image, it\u2019s histogram bimodal",
			"We see the camera man corresponds to the first mode i.e intensities under 87, the second mode is the back round",
			"By setting the threshold accordingly we can segment the camera man",
			"We use the function threshold to perform thresholding,",
			" the output new_image is the image after thresholding has been applied",
			"Along with the image ld the input is the threshold and the max value is set to 255",
			"The final parameter is the threshold type, in this case THRESH_BINARY",
			"this means the output will be 0 or 255. In the output we see the camera has zero values and most of the background has values of 255",
			"We see the histograms intensities are mapped to black and wight after thresholding  has been applied",
			"Sometimes its difficult to select a threshold, therefore we can use OTSU method to select the value automatically.",
			"ret is the threshold value determined,",
			"We see the image looks identical to the one using the manual threshold. Check out the lab for more",
			""
		],
		"is_youtube": false
	},
	"video_009": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: Geometric Operations",
		"unit": "Geometric Operations",
		"video_sources": [
			"https://edx-video.net/43e265c0-4b73-4dcc-8e91-799495f3b430.m3u8",
			"https://edx-video.net/43e265c0-4b73-4dcc-8e91-799495f3b430-mp4_720p.mp4"
		],
		"video_duration": 418,
		"speech_period": [
			4.0, 2.633, 4.0, 8.0, 8.0, 7.5, 4.5, 4.0, 6.167, 3.033, 8.533, 6.1, 7.534,
			3.966, 5.867, 8.0, 4.533, 5.667, 5.033, 4.767, 4.867, 4.0, 7.133, 5.4,
			4.967, 4.433, 5.2, 4.0, 4.0, 4.0, 6.033, 5.967, 4.0, 6.467, 6.6, 5.766,
			6.8, 6.8, 3.567, 8.0, 5.233, 5.1, 5.667, 4.0, 6.633, 5.367, 6.4, 4.933,
			5.1, 4.6, 7.4, 8.634, 8.533, 7.533, 3.767, 7.1, 2.767, 5.966, 6.767, 4.5,
			7.3, 1.9, 6.8, 5.733, 8.5, 8.134, 6.6, 7.033, 5.067, 8.466, 5.8, 5.834,
			4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will discuss Geometric Operations",
			"we will cover: Geometric Operations, Scaling, Translation, Rotation",
			"We will show the one channel representation, but unless specified we can apply these operations to each channel simultaneously",
			"We will treat the image as a function of y and x. y is the vertical direction, x is the horizontal direction.",
			"Although x and y are real numbers, we will sample integer points of x and y.",
			"for example point 0,0 or point 1,1",
			"In a geometric transformation, we change the coordinates of the image x and y.",
			"The new image, g is a function of  x prime",
			"and y- prime g of x prime and y prime has the value of f corresponding to the values that have been mapped from x and y.",
			"We will deal with a subset of Geometric Transformations called Affine transformations",
			"Scaling is where we reshape the image, we can shrink or expand the image in a horizontal and or vertical direction.",
			"Shrinking the image or making the image larger",
			"We can scale the image along the x axis. Let us use \u201ca\u201d to represent  the scaling factor",
			"We can scale the image by 2. g of y and 2x is equal to f of f of y and x,",
			"hence the values of the image g will look stretched relative to image f",
			"Consider the following image, we will only work on the corner pixels points",
			"Consider the image at Point 0,0, the x coordinate is in red",
			"Applying the transform nothing happens, here is point 0, 5",
			"Applying the transform the the x coordinates is mapped from 5 to 10 in red",
			"Applying the transform to point  5,0 nothing happens",
			"the point 5,5 is mapped to 5, 10 overlaying the image points we see the image appears stretched",
			"Let\u2019s look at the relationship between the pixel f and g",
			"the first column of f is mapped to the first column of g (",
			"The second column of f is mapped the third column of g",
			"the final column of f is mapped to the following column in g",
			"The colors shows the relation between columns of image f and g",
			"We see several column\u2019s of g have no corresponding value in  f,",
			"to determine the unknown pixel value we use Interpolation",
			"Interpolation is where we used neighbouring pixels to determine the value of an unknown pixel",
			"In this case, we use the nearest neighbours; this just assigns the value based on the nearest pixel;",
			"there are other methods that PIL and  open cv use",
			"We can scale the horizontal axis; we will denote the vertical scale factor as d",
			"Lets set d to two, nothing changes when applying the transform to the top portion of the image",
			"For the point 5,0, the new value after the transform is applied is mapped to 10, 0",
			"the point 5, 5 is mapped to 10,5. The result is the image is stretched in the horizontal direction",
			"We can use this method to make the image larger, if the values of a or d are less than zero; the image will shrink",
			"Translation is where we shift the image",
			"We can shift an image horizontally by adding the number of pixels \u201ct x\u201d then by mapping the new location x prime",
			"we can add two pixels, consider the following intensity values.",
			"We can apply the transform shifting the points, the points shift",
			"Transforming the next set of points, shifting the points the image appears shifted",
			"Let\u2019s see what happens to each pixel",
			"We see that pixels have been shifted, those pixels that are on the edge have been replaced by zero values",
			"we can increase the size of the image to include the pixels that have been shifted",
			"We can shift the image vertically by adding pixels ty, this shifts the image vertically",
			"We can represent a geometric transformation as a set of equations,",
			"putting the equations in matrix form, we get the\u00a0Affine Transformation matrix",
			"With open cv you input this matrix as an array.",
			" We also have the shear parameters; we will not cover them in this Course. You can also rotate an image",
			"We can rotate an image by an angle theta, where the red lines represent the original orientation of the horizontal and vertical access",
			"We can use a similar matrix to rotate an image. This Rotation matrix will perform a counter-clockwise rotation;",
			"the expression for the matrix is quite complex. We will simplify by assuming the Isotropic scale factor \u201cr\u201d is 1,",
			"and we will rotate from the center of the image to simplify",
			"If you make the above assumptions, libraries like PIL and open CV only require the parameter  theta",
			"Lets try PIL",
			"In PIL we can scale the image by specifying the integer number of pixel\u2019s using the method \u201cresize\"",
			"We can double the width of the image, we apply the method resize the image is twice as wide",
			"You can also shrink the image but the input must be an integer",
			"We can rotate the image. We use the method rotate; the input is the angle we would like to rotate the image by",
			"Lets try open CV",
			"In open cv we can use the function resize to rescale the image. We scale the horizontal axis by two",
			"and leave the vertical axis as is. We can also specify the interpolation",
			"the result is the image is twice as wide. The scaling factor does not have to be an integer, and it can be less than one",
			"Translation requires the Affine Transformation matrix M. Where tx\u00a0is the number of pixels you shift the location in the horizontal direction,",
			"ty\u00a0is the number of pixels you shift in the vertical direction.\u00a0We will leave the scale factors as one",
			"We input the image and matrix into the function warpAffine, we also input the shape of the output image.",
			"The image is shifted, there is a lot more you can do using this method",
			"We can obtain the matrix to rotate the image using getRotationMatrix2D, this will rotate the image by angle \u03b8",
			"The parameter center is the Center of the rotation in the source, scale will be set to one",
			"Like before we rotate the image. Check out the lab for more examples",
			"",
			""
		],
		"is_youtube": false
	},
	"video_010": {
		"section": "Week 2 - Image Processing with OpenCV and Pillow",
		"subsection": "Video: Spatial Operations in Image Processing",
		"unit": "Spatial Operations in Image Processing",
		"video_sources": [
			"https://edx-video.net/701a17b0-e9b1-4c72-a64e-738660f9d529-mp4_720p.mp4",
			"https://edx-video.net/701a17b0-e9b1-4c72-a64e-738660f9d529.m3u8"
		],
		"video_duration": 585,
		"speech_period": [
			4.0, 2.633, 4.767, 5.733, 3.533, 7.367, 7.767, 5.166, 5.467, 3.7, 4.5,
			7.067, 3.2, 4.033, 7.467, 6.633, 5.4, 6.767, 8.766, 8.234, 5.333, 5.767,
			7.333, 8.0, 4.0, 4.0, 7.067, 4.133, 8.8, 9.3, 6.7, 9.767, 8.8, 5.433, 7.4,
			7.467, 3.9, 8.133, 5.8, 7.3, 6.5, 8.467, 8.233, 4.8, 5.933, 6.067, 7.367,
			8.633, 9.0, 6.267, 8.433, 4.3, 7.533, 7.534, 10.6, 6.333, 10.0, 7.867,
			4.933, 9.5, 8.467, 3.233, 4.0, 8.0, 7.2, 8.667, 8.133, 5.2, 5.533, 4.2,
			7.6, 3.734, 8.333, 4.467, 7.933, 5.0, 4.0, 10.067, 7.966, 8.834, 7.633,
			5.033, 0.367, 5.567, 7.366, 0.367, 4.0, 10.3, 5.7, 8.833, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will discuss Spatial Operations in Image Processing",
			"In this video we will review: Convolution sometimes called Linear Filtering,",
			"Edge Detection, Median Filters",
			"In this video we will show the one channel representation, but unless specified we can apply these operations to each channel independently",
			"Spatial Operations consists of a neighbourhood, in this example we take a neighbourhood of 2 by 2 pixels",
			"we apply a function that involves each pixel in the neighbourhood and output the result",
			"we then shift the neighbourhood and repeat the process for each pixel in the image",
			"The result is a new image that has enhanced characteristics,",
			"these spatial Operations take advantage is spatial patterns in the image",
			"Convolution or linear filtering is a standard way to Filter an image the filter is called the kernel",
			"the different kernels perform different tasks",
			"Convolution is analogous to this linear equation",
			"the input is the image X output will be another image Z, we have the parameter W this is known as a kernel or filter",
			"The star this operation is the convolution operation, there is actually an analytical expression for it,",
			"but it's a little confusing, so let's just go over an example to understand what's going on.",
			"here's our image X, here's our kernel w, it\u2019s just a smaller array",
			"The array is made up of values that have been pre determined for some operation. Let's go over the operation of convolution,",
			"We start off in the top right corner of the image, and we'll overlay the kernel with that region of the image. We see the kernel values in red,",
			"we'll multiply every element of the image by the corresponding element of the kernel.",
			"For the first row we'll get the following operation, multiplying  the intensity value and summing the results",
			"this process is repeated for the second row, multiplying  the intensity value and summing the results.",
			"Finally for the final row we multiplying  the intensity value and summing the results, we sum those elements together.",
			"The result is the first element of the image z",
			"We shift the kernel to the left represented by the different colors in red.",
			"We multiply all the elements of the kernel with the image. This gives us the second element of the output image z",
			"We'll shift the kernel one more column and perform the exact same operation,",
			"multiplying every element of the kernel by the corresponding intensity value of the image, adding them all together. This gives us the next value",
			"We then shift the kernel down and repeat the process until we get to the end column. Repeating the process until we get a new image",
			"One problem is the images are different sizes we can fix this my changing the size of the image x",
			"We change the size of the image by padding. In zero padding we add two additional rows of zeros and two additional columns of zeros.",
			"We can also replicate the values of the border. Low Pass Filters are used to smooth the image getting rid of noise",
			"Consider the following image of a square. We have the following noise pixel,",
			"we can reduce this noise by applying a smoothing filter or kernel. Smoothing filters average out the Pixels within a neighbourhood,",
			"they are sometimes called low pass filters. For mean filtering the  kernel simply averages out the kernels in a neighbourhood.",
			"Let's see what happens to the pixel intensities",
			"To explore how the kernel effects different areas we can plot the output image and its relationship to specific regions of this input",
			"when we apply the kernel to this area of the image the intensity values remain unchanged",
			"around the edge of the box the values change as the values of 255 are averaged out with the zeros",
			"Finally, when we get to the region with the noise pixel, we see the noise value is smaller",
			"Comparing the original image and the output image we see the intensity of the noise has reduced but the edges appear less sharp",
			"In general there is a trade off between sharpness and smoothness, in the lab we will review several other kernels that explore this trade off",
			"Edge Detection is an important first step in many computer vision algorithms",
			"Edges in a digital image are where the image brightness changes sharply.",
			"Edge detection uses methods to approximate derivatives and gradients for identifying these areas.",
			"Consider the following image with a vertical and horizontal edge. Let's take a look at the intensity values",
			"Let\u2019s plot the first row of the image, the horizontal axis is the column index, and the vertical axis is the intensity value",
			"If we move in  the right direction from pixel 1 to pixel 2, the intensity value increases. In the image, we can represent this as a vector,",
			"in the same manner, when moving from pixel 2 to pixel 3, the intensity value decreases",
			"we can represent this as a vector pointing in the opposite direction. The direction of the vector represents if the adjacent pixel is increasing,",
			"we can represent this change by applying the following difference equation,",
			"and this works by subtracting the intensity value of the adjacent columns j and j+1 in row I.",
			"This computes an approximation of the gradient in the x-direction. In the following table, each row applies the equation",
			"to the intensity values for column 1 and 2, the final column is the result of using the equation. We can overly the result over the image as vectors",
			"We can apply the same logic to the next column, this time the values are negative",
			"It turns out we can perform similar Horizontal Derivative Approximations using convolution; the\u00a0Horizontal changes\u00a0or gradient are computed by convolving the image with a kernel.",
			"These kernels are called Sobel operators. We can represent the output in the array of intensities G x,",
			"all the values are zero except the elements corresponding to the horizontal edges,",
			"in this image the gray values have different ranges where black is negative, gray is zero, and white is positive. These represent vectors we discussed earlier",
			"We can use the same\u00a0process\u00a0to find vertical changes. This is computed by convolving the image with a kernel to get the image \"G y \"",
			"This time the vectors point in the vertical direction",
			"We can combine the output of each filter into a vector,",
			"if you are familiar with calculus this approximates the gradient. Taking the magnitude of the vectors we get the intensity values,",
			"we can plot it as an image this represents the edges, we can also calculate the angle representing the direction",
			"Median Filters are another popular filter, they are better at removing some types of Noise but may distort the image",
			"The median filter outputs the median value of the neighbourhood, consider the yellow pixel, consider the region in this three by three neighbourhood",
			"the resultant value of the output image is the median value of the 9 pixels",
			"Depending on the padding we see the median is identical to the image in most regions",
			"In the region with the noise sample we see the output is as follows",
			"Overlaying the image values unlike the median filter we see the noise is no longer there and the edges are straight,",
			"but the square is distorted as its missing a pixel",
			"Let\u2019s apply some Spatial Operations in open cv, PIL is relatively simple so check out labs for PIL examples",
			"We will create an image \u201cnew image\u201d that is a noisy version of the original",
			"first we create a  kernel\u00a0for mean filtering. The function filter2D performs 2D convolution between the image",
			"and the kernel on each color channel independently. The parameter values are in the lab",
			"we see the new image has less noise, but the image is blurry",
			"Image Sharpening involves smoothing the image and enhancing the edges. We can accomplish image sharpening by applying the following Kernel",
			"We then apply fillters2D, comparing the image before and after we see it is much crisper",
			"We can perform edge detection consider the following image. We smooth the image using GaussianBlur,",
			"a low pass filter we will be discussing in the lab; this decreases changes that may be caused by noise that would affect the gradient.",
			"We can approximate the derivative in the X or Y direction using the",
			"Sobel",
			"function \u201cddepthis\u201d is the output image depth, dx,",
			"dy is the order of the derivative in each direction in this case it will be one. Finally, \u201ck-size\u201d\u00a0size of the extended",
			"Sobel",
			"kernel; we can output the gradients as images",
			"We can approximate the magnitude of the gradient. We calculate absolute values, and convert the result to 8-bit using convertScaleAbs",
			"Then apply the function\u00a0\u201daddWeighted\u201d\u00a0to calculate the sum of two arrays and assign it to grad",
			"we plot Grad as an image the areas with high intensity values represent edges. Check out the labs for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_011": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Introduction to Image Classification",
		"unit": "Introduction to Image Classification",
		"video_sources": [
			"https://edx-video.net/e7131559-e436-4b08-baa6-3dc5bc1b0fed-mp4_720p.mp4",
			"https://edx-video.net/e7131559-e436-4b08-baa6-3dc5bc1b0fed.m3u8"
		],
		"video_duration": 186,
		"speech_period": [
			4.0, 2.633, 4.0, 2.867, 2.566, 7.334, 3.233, 6.033, 5.1, 6.867, 6.0, 8.0,
			5.633, 8.367, 6.0, 6.8, 8.1, 8.4, 6.867, 7.8, 8.7, 3.3, 7.633, 9.133,
			7.267, 2.0, 4.0, 3.2, 7.6, 5.267, 6.6, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			" This video is an overview of Image Classification",
			"In particular, What is Image Classification",
			"Challenges of Image Classification",
			"Image classification is the process of taking an image or picture and getting a computer to automatically classify it",
			"or by providing the probability of the class of the image.",
			"A class is essentially a label, for instance, cat, car, building and so on.",
			"Image classification has many popular use cases and is used in everyday life",
			"for example if you have a smart phone, we can see some of our pictures organized by friends, families, trips etc.",
			"Image Classification is also used in radiology to help medical professionals identify anomalies in X-rays",
			"Image classification is also used in self driving cars to classify images around them to help the car identify how to navigate the road",
			"We usually start with a sub set of Categories or classes for example cats and dogs",
			"We use the label y to represent  the classes  the image belongs to y=0 for  cat, y =1 for dog",
			"Computers can\u2019t understand images but they can understand the intensity values of a digital image,",
			"we will use the intensity values to classify the image, we let X\u00a0represent the image we want to classify",
			"In the RGB case it\u2019s a three dimension array or tensor, all images must have the same number of rows and columns",
			"We can use the gray scale intensities, we also represent this with X\u00a0 again all images must have the same number of rows and\u00a0columns",
			"A dataset is a set of images x and labels y, here we have a dataset with 7 samples",
			"the comma is used to indicate that they are together, it\u2019s like saying  for image x 4, y  4 equals one i.e a dog",
			"We also have a dataset with more than one class, here is The\u00a0MNIST database\u00a0a large database of handwritten digits,",
			"y can take on the value from 0 to 9",
			"here are160 of the 60,000 small square 28\u00d728 pixel grayscale images of handwritten single digits",
			"The final goal of the module is to come up with a Python\u00a0function\u00a0that takes the input image\u00a0and Outputs a class, but there are many challenges",
			"Challenges of Image Classification, there are many challenges with Image classification consider the cat and dog example",
			"Change in Viewpoint",
			"Change of Illumination. Deformation",
			"Occlusion. Background Clutter",
			"Due to the challenges of Classifying Images we will review several Machine learning Supervised methods for image classification,",
			"k nearest neighbours, Feature extraction, Linear classifiers",
			"Because there  are so many frameworks  we will not cover the code in the videos, check out the labs for code Examples",
			"",
			""
		],
		"is_youtube": false
	},
	"video_012": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Image Classification with KNN",
		"unit": "Image Classification with KNN",
		"video_sources": [
			"https://edx-video.net/8b730845-e7f3-42b4-a2e5-6246f8d09ed6-mp4_720p.mp4",
			"https://edx-video.net/8b730845-e7f3-42b4-a2e5-6246f8d09ed6.m3u8"
		],
		"video_duration": 332,
		"speech_period": [
			4.0, 2.633, 2.7, 4.567, 9.866, 7.7, 7.734, 6.2, 6.1, 4.5, 6.633, 4.0,
			7.633, 4.367, 4.667, 5.7, 4.6, 6.9, 3.666, 5.667, 4.4, 4.4, 6.433, 5.2,
			4.367, 8.0, 6.3, 6.467, 4.766, 6.467, 4.0, 4.0, 4.6, 5.5, 5.533, 6.167,
			6.767, 7.433, 4.867, 3.133, 5.4, 6.6, 7.4, 8.067, 6.266, 5.767, 4.5, 6.4,
			6.3, 7.3, 4.833, 5.3, 5.867, 4.0, 4.0, 9.133, 4.8, 6.967, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"Image Classification with KNN",
			"K NN is a classifier and is  short for K- nearest neighbor.",
			"It is one of the simplest classification algorithms. KNN classifies the unknown data points by finding the most common classes in the k- nearest examples.",
			"It finds the closest match. Now if two points are given on a plane, one set is a class of dogs and",
			"the other set is a class of cat, if an animal is added on the same plane, what will be the group of that animal?",
			"We give the images labels y=0 for cat, y =1 for dog",
			"we will concatenate  the three channel images to a vector or you can do the same for gray scale images",
			"In the video we will use two dimensions so we can visualize what's going on",
			"Let\u2019s use a two dimensional space to represent the images as vectors , we will use colors to represent the classes,",
			"we add a bracket around x to indicate axes of the variable",
			"We can calculate the Euclidean distance between two images as i.e the length of the vector connecting the two points",
			"We have a set of images we will refer to them as training samples.",
			"If we have an unknown sample, we calculate the distance between each image",
			"Lets put the images in a table where each row is a sample, the second column is a class,",
			" the final column is the distance from each sample to the unknown point",
			" We will use the distance to predict the label of the unknown class. As this is a guess or prediction of the class",
			"we use y hat,the hat means its an estimate,",
			"We calculate the distance from our unknown sample, we find the nearest point or nearest neighbour",
			"we assign the label to the unknown sample. We sometimes call this a model,",
			"KNN is simple to code yourself or you can use a software package like sk-learn",
			"We repeat the process for the next unknown sample. How do we know if the model works.",
			"Separating data into training and testing sets is an important part of model evaluation.",
			"We use the test data to get an idea how our model will perform in the real world",
			"When we split a data set, usually the larger portion of data is used for training and a smaller part is used for testing.",
			"For example we can use 70% of the data for training, we then use 30% for testing",
			"we use a training set to build a model, we then use a testing set to evaluate model performance",
			"When we have completed testing our model we should use all the data to train the model",
			"Accuracy shows us how good our method works i,e the average number of times our model got it correct.",
			"Let's use the following table to calculate accuracy on some test data.",
			"The first row in the Table denotes different sample numbers;",
			"the second row shows the actual class label, the third row is the predicted value",
			"and the final row will be one if the sample is predicted correctly or else it will be zero.",
			"We count the number of times the prediction is correct, we then take the average to get the accuracy",
			"Finding the nearest samples does not always work best, consider the following in our 2d space.",
			"The cat is next to a dog that looks like a cat, but all the other samples next to the samples are cats.",
			"To handle this  we find the K closest samples. We then perform majority vote; where within the sub set",
			"we find the class with the most number of samples and assign that label   to the unknown sample,",
			"Let\u2019s do an example where k=3",
			"As before we calculate the distance. We select the K nearest samples",
			"As there are more red samples the new label is assigned to the red class. Let's see how to select k",
			"We use a subset called the validation data to determine the best K, this is called a hyper parameter.",
			"To select the Hyperparameter we split our data set into three parts, the training set, validation set, and test set.",
			"We use the training set for different hyperparameters, we use the accuracy for K on the validation data.",
			"We select the hyperparameter K that maximizes accuracy on the validation set.",
			"We use the test data to see how the model will perform on the real world.",
			"In this course we will combine the validation set and test set to make things simpler. Let\u2019s do an example",
			"In this example we take two samples, we calculate the accuracy for k=1",
			"we calculate the accuracy for k=3, we select  k=3 as it has better accuracy.",
			"If our dataset has enough samples we expect this accuracy in the real world",
			" Once you chose the value of K you can use KNN to classify an image,",
			"let's say you build an app to recognize cats and dogs you let k equal to three",
			"you take the photo, under The Hood your app will calculate the distances",
			"find the nearest neighbours and output the class as a string.",
			"Finally, its simple to add another class, for example fish. The result would look like this in our 2d space",
			"KNN is not usually used in practice, knn is extremely slow,",
			"and it can\u2019t deal with many of the Challenges of Image Classification so let\u2019s learn about other classifiers",
			"",
			""
		],
		"is_youtube": false
	},
	"video_013": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Linear Classifiers",
		"unit": "Linear Classifiers",
		"video_sources": [
			"https://edx-video.net/1eac5940-8e0c-4cae-9779-b8eba5e3095f-mp4_720p.mp4",
			"https://edx-video.net/1eac5940-8e0c-4cae-9779-b8eba5e3095f.m3u8"
		],
		"video_duration": 359,
		"speech_period": [
			4.0, 2.633, 10.633, 8.1, 6.534, 7.3, 4.733, 7.9, 9.1, 7.867, 7.766, 6.634,
			8.133, 6.933, 6.5, 4.6, 3.834, 7.233, 5.133, 5.834, 7.233, 9.9, 8.767,
			0.166, 10.1, 7.834, 5.566, 4.5, 9.034, 8.3, 5.566, 4.6, 8.6, 8.1, 7.3,
			8.9, 9.967, 5.033, 4.834, 9.1, 9.433, 5.467, 6.7, 6.966, 8.134, 9.233,
			9.267, 9.533, 3.267, 5.833, 6.5, 4.0, 0.833
		],
		"transcript_en": [
			"",
			"",
			"In this video we\u2019ll discuss linear classifiers, linear classifiers are widely used in classification and they are the building blocks of more advanced classification methods",
			"In this video we will discuss the two class case we give the images labels, y=0 for cat, y =1 for dog",
			"we will concatenate the three channel images to a vector, or you can do the same for gray scale images",
			"In this video we will show you how a simple function can take an image as an input and output the image class using simple algebra",
			"We will also show you how a similar function will take an image as an input",
			"and output a probability of how likely that image belongs to a class, for example here the function gives a 92% chance it\u2019s a dog",
			"The equation of a line in one dimension is given by the following. Here w represents the weight term and b represents the bias term,",
			"these are called learnable parameters and we will use them to classify the image. For arbitrary dimensions this equation generalizes to a hyperplane,",
			"You can represent  this equation as a dot product of row vector w and image x, this is called the decision plane.",
			"If you're not familiar with vectors this its just a compact way to express the equation of a line in lots of dimensions",
			"In this video we will use 2 dimensions for visualization, note when the letter x is are not bold it's just simple algebra not a sample",
			"We will set the weights  w1 to 1 w2 to -1 and the bias to 1",
			"In 2 dimensions we can plot the equation as a plane, we can plot the plane z=0.",
			"We can see the line where the plane intersects with the plane at  z=0.",
			"If we look at it from above we get the following image",
			"This is the plane where z equals zero, the line is where the decision plane intersects with the plane z=0",
			" this line is called the decision boundary, we can overlay our sample images",
			"Anything on the left side of the line is a dog anything on the right side of the line is a cat",
			"Let's see how we can use the value of z to determine if its a dog or a cat. Let\u2019s look at the following sample",
			"Consider the following values for x 1, we can plug those values into an equation and get a value of  z= 2. We see z is positive",
			"similarly for x2 and x 3, every point on the left side of the line will give you a positive value",
			"",
			"The label for x4 is a cat and the point lies on the rite side of the line plugging the value of x into the equation, we see z is negative",
			"similarly for sample x4  and x 6 every point on the rite side of the line will output a negative value",
			"If we use z to calculate the class of the points it always returns real numbers",
			"such as -1, 3, -2, and so on.",
			"But we need a class between 0 and 1. So how do we convert these numbers? We'll use something called the threshold function pictured here",
			"If Z is greater than 0, it will return a 1, and if Z is less than 0, it will return a 0",
			"For the following image the output, is 1 corresponding to dog",
			"for this input the value is 0 corresponding to cat",
			"Every sample on this side of the plane will be classified as a cat. Every sample on  this side of the plane will be classified as a dog",
			"a plane can't always separate the data. This sample is misclassified in this case, the data set is not linearly separable",
			"The logistic function resembles the threshold function And is given by the following expression known as the sigmoid function",
			"It will give us a probability of how likely our estimate is, in addition it has better performance than the threshold function for reasons we will discuss later",
			"If the value of Z is a very large negative number the expression is approximately 0. And for a very large positive value of Z the expression is approximately 1.",
			"And for everything in the middle the value is between 0 and 1.",
			"To determine \u201cy hat\" as a discrete class we use a threshold shown by the line.",
			"If the output of the  logistic function is larger than 0.5 we set the prediction value yhat to one, if its less we set  \u201cy hat\u201d to 0.",
			"Let's try out some values of X that we used previously with the linear classifier. If we set x equal to point  3, 2  into the equation",
			"We get the value of Z as 2. We pass the value through the sigmoid function",
			"And since the value of the sigmoid function is greater than 0.5, We set yhat to 1",
			"Similarly, if we plug in the value of x as  -2,-3, the value of Z is -2",
			"We pass the value through the sigmoid function we see the result is less than 0.5 so we set \u201cy hat\u201d to 0",
			"We can also represent the logistic function as a probability, we can find the probability of the image of being dog i.e. yhat =1",
			"We can find the probability of being a cat yhat=0 by using the following We see the image is more likely a dog",
			"Given the Following image we can find the probability of being a dog yhat=1, we can find the probability of it being cat that is yhat=0",
			"We see the image is more likely a cat",
			"If you have the learnable parameters you can use a linear classifier. You take the photo",
			"Under The Hood your app will use the linear classifier to make a prediction and output the class as a string.",
			"",
			""
		],
		"is_youtube": false
	},
	"video_014": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Logistic Regression Training: Gradient Descent",
		"unit": "Logistic Regression Training: Gradient Descent",
		"video_sources": [
			"https://edx-video.net/489d2a97-0d7e-4b75-aa0d-241a28bf4682.m3u8",
			"https://edx-video.net/489d2a97-0d7e-4b75-aa0d-241a28bf4682-mp4_720p.mp4"
		],
		"video_duration": 392,
		"speech_period": [
			4.0, 2.633, 7.933, 10.234, 7.033, 6.733, 8.2, 3.8, 4.934, 7.066, 7.4, 4.6,
			6.667, 6.833, 5.4, 9.1, 8.067, 8.367, 7.566, 4.434, 7.266, 3.867, 7.467,
			5.5, 4.266, 9.067, 4.767, 7.133, 7.067, 3.6, 1.0, 1.5, 4.2, 1.0, 2.066,
			5.934, 6.833, 7.0, 5.1, 2.633, 5.134, 6.166, 7.3, 4.0, 4.0, 8.0, 9.867,
			4.433, 5.7, 8.567, 5.433, 4.767, 5.3, 0.633, 4.1, 0.634, 5.933, 4.033,
			8.567, 6.8, 4.067, 8.133, 8.5, 4.0, 8.0, 6.6, 5.4, 5.333, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we\u2019ll discuss Training. In the last video, we learned we could use a plane to automatically classify an image,",
			"in this video, we will learn how to determine the plane, we will use the dataset of images to train the Classifier. When we have an unknown sample we can classify the image",
			"Cost and Loss. Training is where you find the best learnable parameters of the decision boundary.",
			"In this case we can randomly select a set of learnable parameters w and b, the superscript is the guess number.",
			"In this case the decision boundary does a horrible job as it classifies all the images as cats, the second decision boundary does better",
			"Finally this decision boundary performs the best",
			"First we need a way to determine how good are decision boundary is",
			"A loss  function tells you how good your prediction is, the following  loss is called the classification loss",
			"The first column will show the output of the loss function, each time our prediction is correct the loss function will output a zero",
			"each time our prediction is incorrect the loss function will output a one",
			"The cost is the sum of the loss, the cost  tells us how good are learnable parameters are doing on the dataset.",
			"In this case our model output yhat is incorrect predicting a cat as a dog and a dog as a cat.",
			"In this case are model output is correct predicting a dog as a dog and a cat as a cat",
			"For each incorrectly classified samples the loss is one increasing the cost correctly classified samples do not change the cost",
			"For this decision boundary the cost is three, for this decision boundary the cost is 1, for this decision boundary the cost is 0",
			"the cost is a function of the learnable parameters, we see a set of  learnable parameters the decision boundary misclassifies three points",
			"changing the learnable parameters misclassifies the following points, the final learnable parameters perform perfectly",
			"To simplify, let's look at the cost as a function of the bias parameter b",
			"We can plot the cost with respect to learnable parameters,  in this case  we plot the cost  with respect to the bias parameter  b,",
			"lets see the relationship between cost and the decision boundary",
			"we see the first line misclassifies the following points, thus the value of the cost for this value of b is 3.",
			"The second misclassifies the following 2 points hence the value of the cost is 2.",
			"The final lines perform perfectly, the cost is zero",
			"In reality, the cost is a function of multiple parameters \u2018w\u2019 and b, even our super-simple 2d example has to many parameters to plot",
			"In practice, classification error is difficult to work with.",
			"We use the cross-entropy loss that uses the output of the logistics function as opposed to the prediction yhat,",
			"the cost is still the sum of the loss, the cross entropy deals with how likely the image belongs to a specific class",
			"If the likelihood of belonging to an incorrect class is large, the cross entropy loss intern will be large.",
			"cross entropy",
			" loss intern will be large.",
			"If the likelihood of belonging to the correct class is correct",
			"the cross-entropy",
			"is small , but not zero.",
			"Now let's discuss Gradient Descent, a method to find the best learnable parameters",
			"Here's a plot of the cost using cross entropy notice how the curve is smooth compared to the curve classification cost;",
			"if we find the minimum of the cost, we can find the best parameter the gradient gives you the slope of a function,",
			"at any point  we can use gradient descent is a method to find the minimum of the cost function,",
			"let's see how gradient descent works",
			"Consider the cost function. If we start off with a random guess for the bias parameter,",
			"we use the super script to indicate the guess number in this case it is our first guess so it is zero,",
			"we have to move our guess in the positive direction, we can move the parameter in that direction by adding a positive number,",
			"examining the sign of the gradient it is the opposite sign of the number.",
			"Therefore we can add a number proportional to the negative of the gradient",
			"Subtracting the Gradient works if we are on the other side of the minimum in this case, we would like to move in the negative direction",
			"We can move the parameter value to the negative direction by adding a negative number to the parameter. Examining the sine of the Gradient , it is the opposite sine of the number",
			"Therefore we can add an amount proportional to the negative of the Gradient",
			"Here is the final equation of gradient descent, we add a number proportional to the Gradient.",
			"Depending on context  the variable \u201dI\u201d, eta dictates how far we move in the direction of the Gradient it's usually a small number",
			"we see for each iteration, the new values for the bias parameter decrease the cost.",
			"When we reach the minimum, the gradient is zero, the parameter value stops updating",
			"if we use a learning rate that's too low, we may never reach a minimum",
			"if we use a learning rate",
			"that's too large, we may oscillate and never reach the minimum",
			"The \ud835\udc59\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52",
			"is a hyperparameter, we select it by finding a value that has the best accuracy using validation data",
			"We can see the relationship between the cost function and the decision plane;",
			"each iteration of Gradient descent finds a parameter b that decreases the cost, and the decision plane does a better job at separating the classes",
			"it's challenging to perform gradient descent on the threshold function the slope is zero in many regions",
			"if we get stuck in these regions, the gradient will be zero and not update",
			"The decision plane has multiple parameters; as a result, the gradient is a vector, we can update the parameter it\u2019s a set of vectors",
			"for the two dimensional case, we can plot it as a surface; it\u2019s a bowl shape. When we update the parameter it will find the minimum",
			"usually  we plot Cost with respect to each iteration \"i\",",
			"this is called the learning curve. Generally the more parameters you have, the more images and iterations you need to make the model work",
			"Let\u2019s look at different learning curves. We can choose a learning rate that\u2019s way way too large as shown in the one dimensional example",
			"We can choose a learning rate that\u2019s too small, we can choose a learning rate that\u2019s too large",
			"With a good learning rate we will reach the minimum of the cost. That\u2019s it thanks",
			"",
			""
		],
		"is_youtube": false
	},
	"video_015": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Mini-Batch Gradient Descent",
		"unit": "Mini-Batch Gradient Descent",
		"video_sources": [
			"https://edx-video.net/fdfe53ce-7386-4c1f-80ab-2c7b489f93e5-mp4_720p.mp4",
			"https://edx-video.net/fdfe53ce-7386-4c1f-80ab-2c7b489f93e5.m3u8"
		],
		"video_duration": 147,
		"speech_period": [
			4.0, 2.633, 6.367, 8.3, 9.333, 8.067, 7.933, 7.4, 6.733, 9.867, 7.567,
			6.533, 10.633, 10.3, 8.967, 6.4, 11.3, 10.233, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will cover Mini-Batch Gradient Descent, it will allow you to train models with more data",
			"Until now we have been using every sample for gradient Descent.  In Mini-Batch Gradient Descent we use a few samples at a time for each iteration,",
			"it's helpful to think about it as if you are minimizing a mini cost function or the total loss. When we use all the samples in the dataset we call it an epoch",
			"here is epoch 1, here is epoch2. When we use all the samples it\u2019s called batch gradient Descent,",
			"where one iteration equals one epoch. We use a few samples to calculate the cost, it\u2019s sometimes referred to as the total loss,",
			"Let's use the following boxes to represent  the gradient of the total loss at each iteration. Let's do the first epoch.",
			"For the first iteration we use the first two samples. For the second iteration we use the second two samples",
			"For the 3rd iteration we use the last two samples, therefore with a batch size of three to complete one run or Epoch through the data it took 3 iterations",
			"For the second Epoch it also takes three iterations. In this case our batch size is 2.",
			"It only takes two iterations to complete one epoch. For the second epoch it also takes 2 iterations",
			"Let's see how we can determine the number of iterations for different batch sizes and epochs. To obtain the number of Iterations we simply divide the number of training examples by the batch size",
			"Let's verify that. For a batch size of one we get 6 iterations, we can verify this pictorially, we see for each iteration we use one sample",
			"For a batch size of 2 it takes three iterations, we can verify this pictorially. Each iteration uses two samples",
			"Finally, for a batch size of 3 it takes two iterations again we can verify this pictorially",
			"we calculate the total loss for each iteration, it\u2019s  a noisy version of the cost. At the end of each epoch we calculate the accuracy on the validation data",
			"We repeat the process for the next iteration. If the accuracy decreases   we have trained too much. This is called overfitting we will talk about this later",
			"",
			""
		],
		"is_youtube": false
	},
	"video_016": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: SoftMax and Multi-Class Classification",
		"unit": "SoftMax and Multi-Class Classification",
		"video_sources": [
			"https://edx-video.net/e4e42aec-c950-444f-8dd9-01d5e0ddf835-mp4_720p.mp4",
			"https://edx-video.net/e4e42aec-c950-444f-8dd9-01d5e0ddf835.m3u8"
		],
		"video_duration": 210,
		"speech_period": [
			4.0, 2.633, 4.0, 10.067, 5.933, 6.733, 6.0, 6.2, 10.2, 6.267, 7.3, 7.233,
			8.267, 7.233, 6.0, 6.4, 7.634, 6.533, 2.7, 7.467, 3.533, 4.767, 8.633,
			6.5, 7.933, 8.6, 5.867, 4.0, 4.0, 5.367, 7.1, 5.166, 5.334, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we\u2019ll discuss SoftMax and Multi-class Classification",
			"Before we continue let's review the argmax function. The argmax function returns the index corresponding to the largest value in a sequence of numbers.",
			"Here the largest value in z, is 100, and the corresponding index is 0. Thus, the argmax function will return zero",
			"Thus, the argmax function will return zero. In this example, the largest value of Z is 10",
			"and the corresponding index is 7, so the argmax function will return a 7",
			"Logistic regression can be used for two classes but how about if we wanted to solve the following three class problem.",
			"Consider the following images and there label, we have y=0 as cat, y=1  for dog, we have y=2 for  fish",
			"Instead of using one plane to classify the data we will use one plane for each class,",
			"in this case we have three equations representing three classes, but we can generalize to any number of classes",
			"We can also use the  graph to represent  equations, in this case nodes representing the different components of x.",
			"We add nodes for each output z, the edges represents the different learnable parameters with subscripts indicating the dimension",
			"This is the plane where z equals zero , the line is where the decision planes intersects with the plane z=0,",
			"we can overlay our sample images. We see the lines split the classes",
			"If the input is in the blue region, the value of z0 corresponding to the equation zero is the largest,",
			"this is where the blue plane has a hirgher value than the other regions. Therefore anything in this region will be in class zero.",
			"If the input is in the red region, the value of z1 corresponding to equation one is the largest.",
			"Therefore anything in this region will be in class one",
			"If the input is in the yellow region, the value of z2 corresponding to equation 2 is the largest.",
			"Therefore anything in this region will be in class two.",
			"Just a note the yellow line we see is where the plane is grater then zero,",
			"the yellow region is where the yellow plane is larger than the blue and red region. We can now use  the planes to classify this unknown point",
			"We calculate the output of the plane, and apply the arg max function, the result is class 1",
			"The reason the function is called SoftMax since the actual distances i.e. dot products for each input vector with the parameters",
			"is converted to probabilities using the following probability functions. Similar to logistic regression, the process of classification is similar",
			"but we use the output of the probability, we see the result is class 1",
			"Training for SoftMax is almost identical to logistic regression.",
			"Finally there are other ways  to create a multiclass classifier",
			"Sometimes soft max is not the best option for multiclass classification,",
			"We will not go into the details but there are several other methods  to convert a two class classifier to multiclass classifier",
			"they include. One-vs.-rest, One-vs.-one.",
			"These methods are used for support vector machines witch we will discuss in the next section",
			"",
			""
		],
		"is_youtube": false
	},
	"video_017": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Support Vector Machines",
		"unit": "Support Vector Machines",
		"video_sources": [
			"https://edx-video.net/b5302f1e-831d-412c-be5a-caa0e34705d9-mp4_720p.mp4",
			"https://edx-video.net/b5302f1e-831d-412c-be5a-caa0e34705d9.m3u8"
		],
		"video_duration": 374,
		"speech_period": [
			4.0, 2.633, 9.467, 6.533, 8.0, 8.5, 3.5, 9.833, 6.167, 9.0, 5.9, 7.0,
			9.733, 8.367, 6.7, 7.0, 5.3, 8.2, 4.8, 9.5, 7.767, 6.2, 4.533, 8.0, 6.967,
			5.033, 6.833, 5.9, 7.6, 6.567, 5.1, 5.633, 4.9, 7.6, 5.867, 8.0, 7.467,
			3.566, 4.967, 9.8, 10.2, 8.9, 7.9, 5.167, 11.8, 6.233, 9.867, 5.133,
			5.433, 7.567, 6.967, 7.533, 8.367, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will learn a machine learning method called Support Vector Machines (or SVM), which is used for classification. So let\u2019s get started.",
			"In this video we will discuss Kernels, the Maximum Margin. Kernels.",
			"A dataset is linearly separable if we can use a plane to separate each class, but not all data sets are linearly separable",
			"Consider the following data points, the data is not linearly separable we can still see both colors overlap into each other",
			"Same, in this example. The data is not linearly separable",
			"We can \u201ctransform the data\u201d to  make a space where it\u2019s linearly separable. For the sake of simplicity, imagine that our dataset is 1-dimensional data,",
			"this means, we have only one feature x. As you can see, it is not linearly separable.",
			"We can transfer it into a 2-dimensional space. For example, you can increase the dimension of data by mapping x into a new space using a function,",
			"with outputs x and x-squared. Now, the data is linearly separable, right?",
			"Notice that, as we are in a two dimensional space, the hyperplane is a line dividing a plane into two parts where each class lays on either side.",
			"Now we can use this line to classify the dataset. Sometimes its difficult to calculate the mapping",
			"We use a short cut called a  kernel, there are different types, such as: - Linear, - Polynomial, - Radial basis function (or RBF),",
			"The RBF is most widely used. Each of these functions has its own characteristics, its pros and cons,",
			"the RBF kernel finds the difference between two inputs X and X\u2019 that is called a support vector.",
			"The RBF kernel has the parameter Gamma, lets see how we select Gamma",
			"Consider the following dataset of cats and dogs, anything in this region  is a dog, anything in this region  is a cat,",
			"anything in this region is a dog, anything in this region  is a cat",
			"Therefore any sample in the red or blue region should be classified accordingly, unfortunately you can\u2019t find a plane that separates the data",
			"Here we use a  plane to separate a similar  dataset, it does not separate the data, lets try the RBF kernel.",
			"Using a value of gamma of 0.01 increases the flexibility of the classifier",
			"Using a value of gamma of 0.1 seems to be classifying more points",
			"The value of 0.5 classified almost all the points correctly but it does not seem to match the regions in the original slide,",
			"This is known as overfitting, where the classifier fits the data points not the actual pattern,",
			"higher gamma the more likely we will over fit, let's clarify this with an example",
			"The following images of cats look like dogs, they could be mislabeled or the photo could be taken at a bad angle",
			"or the cat could just look like a dog. As a result the image points will appear in the incorrect region",
			"Fitting the model with a high value of gamma we get the following results, the performs almost perfect on the training points",
			"We can represent the classifier with the following decision region, where every point is classified by the color",
			"accordingly this does not match our Decision regions; this is called overfitting",
			"where we do well on the training samples but we may do poorly when we encounter new Data.",
			"To avoid this we find the best value of gamma by using validation data",
			"we split the data into training and validation sets, we use the validation samples to find the Hyperparameters",
			"we test the model for a gamma of 0.5, we get the following misclassified samples,",
			"we see a value for gamma of 0.1 performs better, as a result, we use a value of 0.1",
			"In practice we try several different values of Gamma and select the value that does the best on the validation data",
			"SVM\u2019s work by finding the  Maximum Margin",
			"Witch of the three planes do you think perform better in classifying the data?",
			"Intuitively you would say the green line, you would be correct as changes in the dataset caused by noise would not affect the classification accuracy of the green line",
			"How do we find the best line? Basically, SVMs are based on the idea of finding a plane that best divides a dataset into two classes, as shown here.",
			"As we\u2019re in a 2-dimensional space, you can think of the hyperplane as a line that linearly separates the blue points from the red points.",
			"One reasonable choice as the best hyperplane is the one that represents the largest separation, or\u00a0margin, between the two classes.",
			"So, the goal is to choose a hyperplane with as big a margin as possible.",
			"Examples closest to the hyperplane are support vectors. It is intuitive that only support vectors matter for achieving our goal; and thus, other training examples can be ignored.",
			" We try to find the hyperplane in such a way that it has the maximum distance to support vectors.",
			"Please note, that the hyperplane and boundary decision lines have their own equations. So, finding the optimized hyperplane can be formalized using an equation",
			"which involves quite a bit more math, so we are not going to go through it here, in detail.",
			"That said, the hyperplane is learned from training data using an optimization procedure",
			"that maximizes the margin; and like many other problems, this optimization problem can also be solved by gradient descent,",
			"When the classes are not separable the soft margin SVM can be used.",
			"This is usually controlled by the regularization parameter. This allows some samples to be misclassified",
			"We select gamma and the regulation parameter C by using the values that do best on the validation data, check out the labs for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_018": {
		"section": "Week 3 - Machine Learning Image Classification",
		"subsection": "Video: Image Features",
		"unit": "Image Features",
		"video_sources": [
			"https://edx-video.net/8a9c6b42-519f-426f-8226-ec4718b64406.m3u8",
			"https://edx-video.net/8a9c6b42-519f-426f-8226-ec4718b64406-mp4_720p.mp4"
		],
		"video_duration": 242,
		"speech_period": [
			4.0, 4.0, 8.0, 8.367, 3.633, 4.0, 7.3, 6.5, 5.767, 6.9, 7.366, 8.034,
			6.133, 4.0, 7.833, 8.567, 7.6, 4.0, 6.7, 7.4, 5.9, 4.0, 9.033, 2.967,
			6.267, 7.333, 5.067, 5.633, 7.0, 9.733, 6.267, 8.0, 9.2, 7.467, 5.933, 4.7
		],
		"transcript_en": [
			"In this video we will review Image Features",
			"In practice using the image intensities for classification does not function well.",
			"Here we convert the image to a vector. We use large patches for illustrative purposes instead of individual pixel values",
			"Classifying an image involves the relationship between pixels; a slight change in the image affects this relationship, as discussed in the first chapter.",
			"Consider how a minor shift affects the feature vector",
			"Comparing the two vectors we see the small shift in the image makes the vectors different",
			"Features are measurements taken from the image that help with classification. For example here we have the color histogram",
			"Determining the histogram of the shifted image and eliminating the zeros we see the histograms are similar",
			"But the histograms only count intensities and does not consider the relationships between pixels.",
			"Here we have a circle and a square. The histogram only quantifies the fact that they have the same number of black pixels",
			"One way too overcome this problem, to split image into sub-images and calculate the histogram for each sub-image",
			"Here we have the histogram for the first 5 sub images, here we have the histogram for the second 5 sub images and so on",
			"Color is not always the best tool for classification, consider the task of classifying a circle.",
			"In this case many of the elements of the red channel would have large values",
			"For the blue circle only the elements of the blue or green channel  would have a large magnitude, their histograms have to be the same.",
			"Although humans use colors too classify images it's hard for people to come up with features the use colors, consider the similar circles.",
			"Converting the image to gray scale we see they look similar, surprisingly the gradients look identical",
			"As a result we us features based on gradients",
			"HOG Is one of many image features we can use, it's relatively simple to understand",
			"Histogram of oriented gradients \u2013 H.O.G., is one of many types of features that have been developed over the years",
			"The technique counts occurrences of gradient orientation in localized portions of an image.",
			"HOG would generate a\u00a0Histogram\u00a0for each of these regions separately.",
			"The histograms are created using the gradients and orientations of the pixel values, hence the name \u2018Histogram of Oriented Gradients\u2019",
			"Let's give a basic idea of how HOG works.",
			"Consider the unit circle, we expect the gradians to have the same magnitude",
			"We can calculate the histogram where each bin index is the angle of the gradient, the value of the histogram is the magnitude",
			"We can plot the gradians for the square, the square will have a different histogram",
			"General (H.O.G.) is calculated as follows Assuming we have an image of a kitten.",
			"We will convert the image to grayscale, we calculate the magnitude and angles of the gradients using Sobel",
			"The images are divided in a grid fashion into cells, and for the pixels within each cell, a histogram of gradient directions is compiled.",
			"To improve imbalance to highlights and shadows in the image, cells are block normalized.",
			"The HOG feature vector is a combination of all pixel-level histograms and used with SVM to classify the image.",
			"this example is simplified, we must also consider other free parameters like number of image cells or how many angle bins in the histogram.",
			"There are other types of features for images like SURF and SIFT check out the OpenCV documentation for more",
			"We can summarize the entire machine learning process as follows. Feature extraction,",
			"Kernel i.e non-linear mapping, Linear classification"
		],
		"is_youtube": false
	},
	"video_019": {
		"section": "Week 4 - Neural Networks and Deep Learning for Image Classification",
		"subsection": "Video: Neural Networks",
		"unit": "Neural Networks",
		"video_sources": [
			"https://edx-video.net/63b9847e-890b-42c0-879d-e8956c36705a-mp4_720p.mp4",
			"https://edx-video.net/63b9847e-890b-42c0-879d-e8956c36705a.m3u8"
		],
		"video_duration": 297,
		"speech_period": [
			4.0, 2.633, 2.933, 6.234, 4.8, 6.033, 6.0, 6.0, 6.567, 4.033, 5.4, 4.0,
			4.0, 5.7, 6.3, 8.0, 5.733, 6.267, 5.567, 5.566, 5.3, 4.567, 3.0, 6.467,
			6.0, 6.766, 6.634, 7.966, 3.2, 6.967, 4.0, 5.1, 6.2, 7.2, 5.5, 6.033, 7.2,
			6.767, 4.0, 4.9, 4.0, 4.6, 4.167, 3.333, 7.6, 5.5, 7.033, 6.467, 2.8,
			8.067, 5.533, 4.0, 5.067, 4.4, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will discuss Neural Networks",
			"Consider the following non linearly separable dataset. We will use one dimension for simplicity",
			"Let's look at an example of classification where we overlaid the color over the feature,",
			"in the context of neural networks its helpful to think of the classification problem as a decision function",
			"just like a function when y equals one the value is mapped to one on the vertical axis",
			"we can represent the function like a box , this box function is an example of a decision function",
			"any values of x in the following region is one, any value of x in this region is mapped to zero",
			"A neural network will approximate this function using learnable parameters",
			"We can also view the problem as trying to approximate the box function using logistic regression",
			"anything in this region y will be a one i.e dog",
			"anything in this region y will be a 0 i.e cat",
			"If this was our cat dog dataset In this example we cannot use a straight line to separate the data",
			"This line can be used to linearly  separate some of the data, but some of the data is on the wrong side of the line",
			"we can use the following node to represent the line and the edges to represent the input x and output  z",
			"If we apply the logistic function, in the context of neural networks this is called the activation function",
			"These values of the function are incorrect and we get an incorrect result  in this region",
			"We can represent the sigmoid function with the following node taking the input z from the linear function",
			"and producing an output, technically  \u201cA\u201d is a function of z and x",
			"We will call the function \u201cA\u201d the activation function and the output of  of  \u201cA\u201d  is called the activation",
			"This line can also be used to linearly separate some of the data",
			"but some of the data is on the wrong side of the line",
			"this line looks like it can be use to separate the data but lets see what happens when we apply the sigmoid function",
			"After applying the sigmoid or activation function, we get an incorrect result for some of the samples",
			"Consider the following sigmoid functions we call them \"A sub script one\u201d and \u201cA sub script two\u201d",
			" If we subtract the second sigmoid function from the first sigmoid function we get something similar to the decision function",
			"We can also apply the following operations with a linear function i.e just subtract the second activations from the first activation function.",
			"These values will be learnable parameters",
			"If we apply a threshold setting every value less than 0.5 to zero and grater than 0.5 to one",
			"we get the exact function we are trying to approximate",
			"We can now classify the data, we obtain the parameters via gradient descent",
			"we can use the  graph to represent the process, we apply two linear functions to x and we get two outputs",
			"to each linear function we apply a sigmoid we then a apply a second linear function to the outputs of the sigmoid",
			"we usually apply another function to the output of this linear function then apply a threshold",
			"This diagram is used to represent a two-layer neural network, we have the hidden layer",
			"Each linear function and activation is known as an artificial neuron, in this case the hidden layer has two artificial neurons",
			"the output layer has one artificial neuron, as it has two inputs the input dimension for this neuron is two",
			"Its helpful to look at the output components of the activation,",
			"the outputs of the activation function is a 2D plane that looks like this",
			"these red data points get mapped to these points  in the 2D plane",
			"these blue  data points get mapped to these points  in the 2D plane and so on",
			"It turns out that we can split the point using the following plane",
			"this is what the linear function on the second layer does",
			"In the same way we can add more dimensions to the input, notice that there are a lot more weights between the input layer and hidden layer,",
			"we will leave out the bias terms, we see a neural networks had a lot of learnable parameters,",
			"for example a logistic regression model may have hundred's  of learnable parameters a modern neural network will have millions.",
			"Generally these type of Neural Networks are called Feedforward Neural Networks or fully connected networks,",
			"but we will usually refer to them as Neural Networks",
			"We can use neural networks to classify multiple dimensions here we have a non-linearly separable dataset  in two dimensions",
			"Here we have a neural network with 2 dimensions. The more dimensions the more Neurons  we require",
			"We can plot the decision function in 2 dimensions,",
			"the horizontal axis is the value yhat, we have cats that are mapped to zero",
			"we have dogs that are mapped to one. Check out the labs for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_020": {
		"section": "Week 4 - Neural Networks and Deep Learning for Image Classification",
		"subsection": "Video: Fully Connected Neural Network Architecture",
		"unit": "Fully Connected Neural Network Architecture",
		"video_sources": [
			"https://edx-video.net/daaeb6ca-ce25-44b9-a17f-d0a614ce34d1-mp4_720p.mp4",
			"https://edx-video.net/daaeb6ca-ce25-44b9-a17f-d0a614ce34d1.m3u8"
		],
		"video_duration": 239,
		"speech_period": [
			4.0, 2.633, 8.0, 4.0, 5.533, 5.867, 4.6, 6.967, 4.633, 5.733, 6.934,
			7.733, 4.0, 4.733, 4.9, 7.134, 5.5, 4.366, 5.367, 4.0, 4.0, 4.6, 6.233,
			3.967, 5.2, 4.833, 5.867, 5.933, 3.367, 5.767, 6.233, 4.0, 4.0, 5.267,
			4.233, 3.867, 5.133, 5.967, 4.833, 7.667, 3.033, 4.0, 5.0, 4.467, 4.6,
			5.666, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will cover Fully Connected Neural Network Architecture, This deals with how to arrange the different number of hidden layers and neurons.",
			"neural networks are usually represented with out the learnable  parameters",
			"in this example the hidden layer has four neurons, the output layer has one neuron",
			"We can make multiclass predictions using neural networks, we just add more neurons to the output layer.",
			"The process can be  thought of as  just replacing the output layer with a SoftMax function,",
			"here the output  layer has three neurons for three classes. For a given  input we obtain an output for each neuron",
			"We choose the class according to the index of the neuron that has the largest value.",
			"In this case neuron two has the the largest value, so the output of our model is two",
			"We can use the following diagram we have 5 neurons in the output layer, one for each class dog, cats and so on",
			"We can add hidden layers, here we Have two hidden layers, if we have more than one hidden layer the neural network is called a deep neural network.",
			"More neurons or more layers may lead to overfitting",
			"The output or activation of each layer is the same dimension as the number of neurons,",
			"this layer has three neurons, the output or activation has three dimensions",
			"Each neuron is like a linear classifier, therefore each neuron must have the same number of inputs as the previous layer.",
			"In this case the previous layer has three neurons, so this neuron has three inputs",
			"Let\u2019s see how the following neural network makes a prediction in the following layers,",
			"consider the following input vector with four dimensions. Each neuron in the 1st layer has 4 inputs",
			"as there are three neurons, the activation has a dimension of three",
			"each neuron in the next layer has an input dimension of 3",
			"as there is 2 neurons in the second layer the output activation has a dimension of two",
			"One way to select a Fully Connected Neural Network Architecture is to use the validation data",
			"consider the following network Architecture  A , B and C",
			"we select the Architecture with the best performance  on the validation  data, in this case C",
			"It turns out that deep networks work better, but are hard to train",
			"If you recall to perform Gradient descent to obtain our learning parameters we have to calculate the gradient,",
			"but the deeper the network the smaller the gradient gets this is called the vanishing gradient.",
			"As a result its harder to train the deeper layers of the network",
			"One of the main drawbacks with using the sigmoid activation function is the vanishing gradient",
			"One activation function is the rectified linear unit function or Relu function for short",
			"The value of the relu function is 0 when its input is less then zero.",
			"Relu is only used in the hidden layers",
			"if the input z is larger than 0 the input of the function will equal its output",
			" if the input z equals 5, the output equals 5",
			"if the input z equals 10, the output equals 10",
			"Networks have layers that help with training, we will skip the details, but",
			"some methods like dropout  prevent overfitting, batch normalization to help with training",
			"skip connections allow you too train deeper Networks by connecting deeper layers during training",
			"The hidden Layers of Neural networks replace the Kernels is SVM\u2019s.  We can use the raw  Image or features like HOG",
			"Training neural networks is more of an art than a science,",
			"so we will use the lab to try out different methods, generally",
			"Neural networks are trained in a similar manner to logistic regression and Softmax",
			"The Loss or cost surface is complicated making training difficult",
			"In the lab: we will explore more advanced variants of gradient descent",
			"Other Advanced Methods to prevent overfitting. That's it, check out the lab for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_021": {
		"section": "Week 4 - Neural Networks and Deep Learning for Image Classification",
		"subsection": "Video: Convolutional Networks",
		"unit": "Convolutional Networks",
		"video_sources": [
			"https://edx-video.net/96004af2-6b89-4f68-8db4-73b0770e250d-mp4_720p.mp4",
			"https://edx-video.net/96004af2-6b89-4f68-8db4-73b0770e250d.m3u8"
		],
		"video_duration": 393,
		"speech_period": [
			4.0, 2.633, 5.3, 7.033, 7.034, 5.266, 3.934, 4.566, 6.867, 4.867, 6.033,
			5.1, 1.567, 6.433, 5.8, 3.233, 7.667, 3.3, 5.1, 4.233, 6.667, 8.933,
			7.767, 8.233, 8.434, 8.4, 4.833, 5.767, 5.733, 5.133, 4.767, 6.933, 5.5,
			7.567, 7.333, 3.2, 3.834, 4.6, 5.8, 4.366, 6.867, 3.2, 6.5, 6.3, 4.0, 6.3,
			5.7, 6.733, 7.567, 8.067, 1.633, 6.033, 5.134, 4.833, 4.0, 4.6, 9.033,
			9.9, 4.0, 4.0, 7.534, 8.7, 5.333, 4.267, 3.033, 7.067, 5.166, 8.734, 4.0,
			1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video, you will learn about deep Convolutional Networks CNN\u2019S for short",
			"In this Video we Will review: 1. How CNN\u2019s Build Features 2.Adding Layers",
			"3.Receptive Field 4.Pooling 5.Flattening and the Fully Connected Layers",
			"A convolutional network or CNN pictured here is a neural network with special layers,",
			"the model classifies an image by taking a part of the image,",
			"each input image will pass it through a series of convolution layers with filters,",
			"pooling layers, fully connected layers and while applying activation functions to classify an object.",
			"Convolution and pooling layers are the first layers used to extract features from an input t",
			"these can be thought of as the feature learning layers, the fully connected layers are simply a neural network",
			"Both are learned simultaneously by minimizing the cross-entropy loss",
			"How CNN\u2019s Build Features",
			"If you recall the H.O.G feature  used Sobel kernels to detect vertical  and horizontal edges,",
			"Looking at the kernels we see the vertical edge detector kernel looks like a vertical edge",
			"and the horizontal edge looks like a horizontal edge",
			"We can represent H.O.G with  a diagram that looks similar to a neural network, we replace the linear function with a convolution",
			"and we have the squaring and square root operations",
			"In a CNN we have neurons but the kernels are learnable parameters.",
			"The activation Functions in this case RELU are applied to each pixel,",
			"instead of an activation the output is an activation map or feature map,  similar to a one channel image",
			"Like the HOG\u2019s Sobel kernel each kernel of a CNN will detect a different property of the image, for example this activation map shows this kernel",
			"detects regions around the dogs eyes. This kernel picks up regions around the mouth, usually each output channel is smaller",
			"We use multiple kernels analogous to multiple neurons, if we have M Kernels we will have M feature maps",
			"for each map we apply the Convolution + RELU. Generally we will use clear squares to represent the feature or activation maps",
			"A gray scale image Can be seen as a 1 channel input, if we have M kernels, each feature map will be a channel,",
			"therefore we will have M output channels. Adding Layers",
			"We can also stack convolutional layers each output channel is analogous to the neurons,",
			"like a neuron the input of the next layer is equal to the output of the previous layer.",
			"Here we have three outputs, the next layer will take three outputs as Inputs,",
			"if this layer has two outputs it will output two feature maps",
			"the next layer will apply a convolutional kernel to each input, then add them together, then apply an activation function.",
			"The neurons are replaced with kernels. The previous layer has three  output channels,",
			"for the first input we apply the convolution and activation to the output of the first channel, we obtain the feature map",
			"we repeat the process for the second input channel adding it to the activation map, repeating for the third input channel",
			"The process is repeated, for the next input",
			"We can process color images by having three input channels",
			"Just like a neural network we can stack layers, using the 3d representation,",
			"we can also represent them with these yellow boxes indicating the kernel size and the number of channels",
			"It\u2019s helpful to look at the kernels to understand what the different layers are doing,",
			"if you recall the Sobel kernels looked like they detect vertical and horizontal edges they were trying to detect.",
			"Consider a CNN used to see faces,",
			"the kernels in the first layer look like edges and corners, the second layer looks like parts of the face",
			"the final layer looks like faces. We see that adding more layers builds more complex features.",
			"Receptive Field is another important factor in CNNs",
			"Receptive Field is the size of the region in the input that produces a pixel value in the activation map",
			"consider the following pixel in the following activation map, its Receptive Field is given by",
			"the larger the Receptive Field the more information the activation map contains about the entire image",
			"We can increase the receptive field by adding more layers, this requires less parameters then increasing the size of the kernel",
			"If we have one layer the receptive field would be the following region. Adding a second layer increases the receptive field further",
			"Pooling",
			"Pooling helps to reduce the number of parameters, increases the receptive field while preserving  the important features.",
			"To make it easier to understand, we can think about it as resizing the image.",
			"Max pooling is the most popular type of pooling. Consider the feature map.",
			"We Apply Max pooling with dimensions 2x2,",
			"it takes the maximum pixel value and we get a smaller feature map",
			"Pooling also makes CNN's more immutable to small changes in the image we have two images they are identical but one slightly shifted",
			"let's see the activation map after max pooling, we apply max pooling in first region and repeat",
			"We see the activation map output is now identical",
			"Finally we have Flattening and the Fully Connected layers",
			"We simply flatten or reshape the output of the Feature Learning layers and use them as an input to the fully connected layers",
			"for example if the output of the max pooling layer is 7 units of width and 7 units of height, we flatten or reshape the output,",
			"this is analogous to a feature vector this is the input to the fully connected layer",
			"each neuron has the input dimension as a flatten output",
			"for more channels we apply the similar procedure",
			"if we have 32 output channels each channel is 4x4 for a total of 16 elements",
			"as there are 32 channels multiplied by 16 we have a total of 512 elements,",
			"we flatten or reshape the output to have 512 outputs  as a result each neuron will have 512 input dimensions",
			"",
			""
		],
		"is_youtube": false
	},
	"video_022": {
		"section": "Week 4 - Neural Networks and Deep Learning for Image Classification",
		"subsection": "Video: CNN Architectures",
		"unit": "CNN Architectures",
		"video_sources": [
			"https://edx-video.net/e700d947-07cf-4285-817b-19f004b0b8a6-mp4_720p.mp4",
			"https://edx-video.net/e700d947-07cf-4285-817b-19f004b0b8a6.m3u8"
		],
		"video_duration": 252,
		"speech_period": [
			4.0, 2.633, 4.0, 8.733, 4.967, 4.567, 5.733, 10.0, 7.8, 6.767, 9.366,
			3.267, 6.8, 7.067, 8.4, 4.533, 4.0, 6.233, 6.034, 7.0, 8.166, 8.0, 5.567,
			8.833, 5.467, 6.333, 7.867, 8.333, 4.967, 4.1, 6.433, 7.8, 4.6, 5.634,
			5.366, 6.567, 4.7, 9.4, 7.267, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video, you will learn about different CNN Architectures",
			"popular architectures include : LeNet five, AlexNet, VGGNet, ResNet",
			"We will go over a few of them in this session. We will also cover Transfer Learning",
			"One of the first CNNs was proposed by Yann LeCun et al. in 1989",
			"The most successful use case of the LeNet-5 is the MNIST Data set of handwritten digits",
			"LeNet receives an input image, normally a grayscale image, it uses a 5 by 5 filter with a stride 1 and results in a volume of 28 by 28 outputs.",
			"The next layer is a pooling layer with 14 by 14 outputs. It repeats itself with a filter and pooling layer till it gets to the",
			"fully connected layers where it flattens to create 120 neurons and another with 84 neurons",
			"while using a sigmoid activation function to produce an output. For a while CNNs  dropped out of popularity for image classification,",
			"and support vector machines became the default standard",
			"If you want to compare any image classification methods you compare the classification accuracy on a dataset.",
			"Image Net is a benchmark dataset i.e this is the one that everyone uses to see who has the best image classification method.",
			"Here is an image of the top performer every year. Prior to 2012 a method using SIFT a feature like hog",
			"took the top spot  with 51 precent accuracy, after 2012",
			"Alex Net  smashed this record with 63.3%\u00a0accuracy.",
			"This jump was so large everyone started using CNNs for image classification ,",
			"Here is a block diagram of Alex Net we see the network has lots of parameters,",
			"we see convolution kernels are of different sizes. If you recall more parameters means  you require more data.",
			"We see the first  convolution kernel layers are of  shape 11x11 with 25 channels, this is quite a large number of parameters",
			"The VGG Network  is a Very Deep Convolutional Network that was developed out of the need to reduce the number of parameters in the Convolution layers",
			"and improve on training time, it also showed in general that deeper networks performed better",
			"VGGNet has multiple variants like the VGG 19 and VGG 16, where 16 stands for the number of layers in the network.",
			"Here VGG-16 is pictured next  to Alex net, we see it\u2019s much deeper.",
			"The key insight gained by the VGG networks is we could replace the larger kernels in the convolution layer",
			"by stacking convolution layers with 3 by 3 kernels, keeping the same receptive field while reducing the number of parameters.",
			"This also reduced the number of computations by reducing the number of operations in the convolution layers and decreasing the feature map size",
			"As CNNs got deeper the vanishing gradient began to become a problem again",
			"ResNet help solve the problem by introducing Residual learning:",
			"Residual layers or skip connections allows the gradient to bypass different layers improving performance",
			"we can now build much deeper Networks, here is a 32 layer network from the paper Deep Residual Learning for Image Recognition",
			"Transfer learning is where you use a pre trained cnn to classify an image",
			"Instead of building your own network you can use the CNN Architectures we discussed,",
			"Pre-trained CNN \u2018s have been  trained with vast amount of data. We will cover the simplest method,",
			"where we replace the SoftMax layer with our own SoftMax layer. The number of neurons is equal to the number of classes,",
			"we then train the SoftMax layer on the dataset we would like to classify",
			"the input dimension of each neuron in the SoftMax layer is equal to the last number of neurons  fully connected layer. The Pretrained model can be thought of as a feature generator,",
			"depending on the size of your dataset you can use SVM instead of the SoftMax layer. Check out the labs. for more",
			"",
			""
		],
		"is_youtube": false
	},
	"video_023": {
		"section": "Week 5 - Object Detection",
		"subsection": "Video: Object Detection",
		"unit": "Object Detection",
		"video_sources": [
			"https://edx-video.net/821d1dbc-a88f-49c1-a609-f0bc9b749a53-mp4_720p.mp4",
			"https://edx-video.net/821d1dbc-a88f-49c1-a609-f0bc9b749a53.m3u8"
		],
		"video_duration": 306,
		"speech_period": [
			4.0, 2.633, 3.367, 1.1, 4.0, 4.0, 4.0, 6.6, 4.866, 5.567, 4.333, 7.3,
			4.467, 3.867, 4.2, 3.4, 4.2, 4.3, 2.9, 1.866, 4.434, 4.633, 5.833, 4.2,
			5.767, 3.0, 6.267, 7.466, 4.534, 4.0, 4.0, 4.366, 8.567, 5.233, 3.767,
			7.033, 2.367, 4.667, 3.3, 6.3, 2.4, 4.433, 7.567, 6.833, 6.7, 3.333,
			4.367, 4.4, 7.167, 6.266, 3.534, 5.4, 4.0, 4.533, 3.467, 4.0, 4.0, 5.966,
			6.767, 3.267, 4.4, 5.3, 5.7, 6.766, 4.0, 1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video, you will learn about Object Detection",
			"In particular",
			"Sliding Windows, Bounding Box",
			"Bounding Box Pipeline, Score",
			"Image Classification predicts the class of an object in an image",
			"Classification and Object Localization locate the presence of an object and indicate the location with a bounding box",
			"Object detection locates multiple objects with a bounding box and their classes",
			"In object detection problems, we generally have to find all the possible objects in the image",
			"to achieve this, we use an algorithm known as\u00a0Sliding window detection",
			"If we want to detect a dog we consider a fixed window size, if chosen properly, the dog will occupy most of the window,",
			"this is essentially a sub image that we would like to classify as a dog",
			"the other sub images would be classified as background",
			"each image that does not contain the dog would be considered a background class",
			"The sliding window algorithm is a more systematic approach",
			"We start in one region in the image, classify that Sub-image",
			"We then shift the window and classify the next sub-image",
			"We repeat the process",
			"When we get to the horizontal border",
			"We move a few pixels down in the Vertical direction and repeat the process",
			"When the object occupies most of the window, it will be classified as a dog.",
			" But in addition to the problems with classification there are  additional problems  specific to object detection",
			"Object detectors often output many overlapping detections",
			"We also have the issue of object sizes, where the same object can come in different sizes",
			"(click 1) one way to solve this is to reshape the image",
			"The same object can have different shapes, again reshaping or resizing the image is one solution",
			"We also have the problem of Overlapping objects in pictures, so as such this may pose issues to the sliding windows.",
			"Bounding Boxes is another method for object Detection, it can be used independently,",
			"with sliding Windows or with other more advanced methods",
			"The bounding box is a rectangular box that can be determined,",
			"with\u00a0the lower-right corner of the rectangle with coordinates y 0 and x. 0",
			"and the width and height.The y and x are not the same as the classification labels y and the image x so  we will color them blue",
			"It can also be determined by the coordinates in the upper-left corner  y-min and x-min",
			"and the lower-right corner  the\u00a0x-max and\u00a0y-max.",
			"Remember these are not the labels and the image they are just to illustrate the coordinates of the Bounding box we will call box",
			"Lets do an  example",
			"The coordinates in the upper-left corner is given by 0,1",
			"and the lower-right corner is 7,8",
			"The goal of object detection is to predict these points, so we add a \u201chat\u201d to indicate it\u2019s a prediction",
			"The Bounding Box Pipeline",
			"Like classification, we have the class y and x.",
			"We also have the bounding box, just like classification  we have a dataset of Classes and their Bounding boxes",
			"Similar to classification, we use the dataset to train the model, we include the box coordinates,",
			"the result is an object detector with updated learning parameters we will discuss the specific models and training later",
			"we input the image with the objects we would like to detect",
			" we have the predicted class and the box coordinates, in this case a dog",
			"We also have the predicted class cat and the box coordinates.",
			"The predicted class bird and the box coordinates and another class predicted as bird and the box coordinates",
			"Many object detection algorithms provide a score letting you know how confident the model prediction is",
			"Each column In the table has an image and it\u2019s Prediction",
			"The first row is the score ranging from 0 to 1, the second row is the class",
			"And the third row is. the image and its bounding box.",
			"For the first row we see  the prediction is dog, but the image does not look like a dog,",
			"as a result the score is low 0.5.",
			"This means the model is not confident about its prediction",
			"For the second row we see the prediction Is dog and the image looks like a dog,",
			"as a result the score is  0.99 in this case, the model is confident about its prediction",
			"For each detection a score is provided, we can adjust so we only accept detections above a specific score,",
			"here we have detected one dog and two cats.",
			"It looks like we detected both a cat and a dog in the dogs location",
			"Examining the score we see that one of the cat prediction\u2019s has a low score of 0.5.",
			"If we  only accept scores above 0.9 we correctly detect the cat and dog.",
			"Usually models will only output objects over a specific threshold. Check out the labs for more examples",
			"",
			""
		],
		"is_youtube": false
	},
	"video_024": {
		"section": "Week 5 - Object Detection",
		"subsection": "Video: Object Detection with Haar Cascade Classifier",
		"unit": "Object Detection with Haar Cascade Classifier",
		"video_sources": [
			"https://edx-video.net/dee58ca8-7ed4-49b4-aa32-74e20beabfb9-mp4_720p.mp4",
			"https://edx-video.net/dee58ca8-7ed4-49b4-aa32-74e20beabfb9.m3u8"
		],
		"video_duration": 263,
		"speech_period": [
			4.0, 2.633, 6.567, 4.066, 5.367, 4.0, 5.933, 6.067, 5.667, 6.9, 4.133,
			6.267, 3.233, 8.9, 7.567, 5.333, 10.033, 7.4, 8.334, 5.366, 5.667, 8.933,
			9.2, 7.734, 3.066, 8.0, 3.434, 9.933, 8.1, 3.633, 5.167, 6.633, 5.367,
			4.0, 7.533, 3.8, 4.0, 4.667, 4.0, 5.867, 6.133, 5.567, 7.033, 3.067, 4.0,
			1.0
		],
		"transcript_en": [
			"",
			"",
			"In this video we will provide an overview of Object Detection using\u00a0the Haar feature-based Cascade Classifiers.",
			"We are going to use Haar Feature-based Cascade Classifiers to detect",
			"Cars, traffic lights, pedestrian stop signs etc in this image.",
			"The method was proposed by P. Viola and M. Jones in 2001.",
			"It is a machine learning method where a cascade function is trained on a large number of positive images",
			"which means that it includes the object we are trying to detect and negative images i.e the background",
			"Paul Viola and Michael Jones used the idea of the Haar wavelets in the Haar Feature Classifier.",
			"After millions of training images are fed to the system, the classifier begins by extracting features from each image.",
			"Haar wavelets are convolution kernels used to extract features",
			"Haar wavelets extract information about: Edges, Lines, Diagonal edges",
			"This example we overlay the Haar wavelets  over the car",
			"One - The Integral image concept is each pixel represents the cumulative sum of the corresponding input pixels above and to the left of that pixel.",
			"The concept takes in the pixels of an input image as such as this. To get the integral sum for the highlighted pixel,",
			"we will add everything to the left and to the top, since there is nothing to the left and top,",
			"We will get a sum of 1 by just adding itself. To get the value of the 2nd pixel highlighted, we will add 1 which is to the left and itself 2 and get 3",
			"Lets try this once more, to get the integral sum of the highlighted pixel, we will get the sum of 2,",
			"4, 1, 3, 2 and itself, 3. We will get 15.",
			"The Viola-Jones paper used a 24 by 24 base window size as an example",
			"and that would result in more than 180,000 features calculated in the integral image.",
			"Two - The algorithm selects a few important features from a large set to give highly efficient classifiers by employing the use of an AdaBoost,",
			"The idea is to set weights to both classifiers and samples in a way that forces classifiers to concentrate on observations that are difficult to correctly classify.",
			"Therefore, it selects only those features that help to improve the classifier accuracy by constructing a strong classifier",
			"which is a linear combination of weak classifiers.",
			"In the case of the 24 by 24 window example used by Viola-Jones, over 180000 features were generated.",
			"Using the AdaBoost cuts it down to about 6000 features.",
			"Two - The algorithm selects a few important features from a large set to give highly efficient classifiers by employing the use of an AdaBoost,",
			"Let us illustrate with cats and dogs. Each weak classifier splits the examples with at least 50% accuracy,",
			"the misclassified examples are then emphasized on the next round.",
			"The idea is to set weights to both classifiers",
			"and samples in a way that forces classifiers to concentrate on observations that have been misclassified.",
			"The process is repeated until it has minimized the number of errors and constructs a strong classifier.",
			"A strong classifier is a linear combination of weak classifiers.",
			"In the case of the 24 by 24 window example used by Viola-Jones, over 180,000 features were generated.",
			"Using the AdaBoost cuts it down to about 6000 features.",
			"Three - Cascades of classifiers are then used.",
			"This classifier groups sub-images from the input image in stages",
			"and disregards any region that doesn\u2019t match the object it is trying to detect",
			"To detect the car in this image, the classifier groups the features into multiple sub-images",
			"and the classifier at each stage determines whether the sub-image is the object we are trying to detect.",
			"In the case that it is not, the sub-window is discarded along with the features in that window.",
			"If the sub-window moves past the classifier, it continues to the next stage where the second stage of features is applied",
			"until it is sure that it is a car.",
			"",
			""
		],
		"is_youtube": false
	}
}
